{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddp8ZBuuh1R2"
      },
      "outputs": [],
      "source": [
        "### Install dependencies\n",
        "!pip install datasets\n",
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import spacy\n",
        "from openai import AzureOpenAI\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Configuração do cliente base\n",
        "client = AzureOpenAI(\n",
        "  azure_endpoint=\"<azure_endpoint>\",\n",
        "  api_key=\"<api_key>\",\n",
        "  api_version=\"2024-02-01\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZeOCYJrSiCyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window(text, window_size=400, overlap=50):\n",
        "    \"\"\"\n",
        "    Function that splits a text into sliding windows of size `window_size` with an `overlap`.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be processed.\n",
        "        window_size (int): The maximum number of tokens per window.\n",
        "        overlap (int): The number of overlapping tokens between windows.\n",
        "\n",
        "    Returns:\n",
        "        List of str: List of texts divided into sliding windows.\n",
        "    \"\"\"\n",
        "    # Load SpaCy model for Portuguese (or another language, if necessary)\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    # Process the entire text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract tokens\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    # List to store text windows\n",
        "    windows = []\n",
        "\n",
        "    # Sliding window implementation\n",
        "    for i in range(0, len(tokens), window_size - overlap):\n",
        "        # Capture a window of tokens\n",
        "        window = tokens[i:i + window_size]\n",
        "        windows.append(\" \".join(window))\n",
        "\n",
        "        # Stop if we are at the end of the text\n",
        "        if i + window_size >= len(tokens):\n",
        "            break\n",
        "\n",
        "    return windows\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_tagged_words(text):\n",
        "    \"\"\"\n",
        "    Extract labeled words from text based on XML-like tags.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text containing tags.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries with extracted words, their category, subcategory,\n",
        "              and character positions in the original text.\n",
        "    \"\"\"\n",
        "    dict_categories = {\"AGE\": \"AGE\", \"PHONE\": \"CONTACT\", \"FAX\": \"CONTACT\", \"EMAIL\": \"CONTACT\", \"URL\": \"CONTACT\",\n",
        "                   \"IP_ADDRESS\": \"CONTACT\", \"DATE\": \"DATE\", \"IDNUM\": \"ID\", \"MEDICAL_RECORD\": \"ID\", \"DEVICE\": \"ID\",\n",
        "                   \"HEALTH_PLAN\": \"ID\", \"BIOID\": \"ID\", \"STREET\": \"LOCATION\", \"CITY\": \"LOCATION\", \"ZIP\": \"LOCATION\",\n",
        "                   \"STATE\": \"LOCATION\", \"COUNTRY\": \"LOCATION\", \"LOCATION_OTHER\": \"LOCATION\", \"ORGANIZATION\": \"LOCATION\",\n",
        "                   \"HOSPITAL\": \"LOCATION\", \"PATIENT\": \"NAME\", \"DOCTOR\": \"NAME\", \"USERNAME\": \"NAME\", \"PROFESSION\": \"PROFESSION\",\n",
        "                   \"OTHER\": \"OTHER\", \"LOCATION\": \"LOCATION\"}\n",
        "\n",
        "    pattern = r\"<(.*?)>(.*?)</\\1/>\"  # Regex to capture tags and content between them\n",
        "    matches = re.finditer(pattern, text)\n",
        "\n",
        "    result = []\n",
        "    for match in matches:\n",
        "        tag = match.group(1)\n",
        "        word = match.group(2)\n",
        "        first_position = match.start(2)  # Start position of the extracted word\n",
        "        last_position = match.end(2)     # End position of the extracted word\n",
        "        category = dict_categories.get(tag, \"UNKNOWN\")  # Main category\n",
        "        subcategory = tag\n",
        "\n",
        "        result.append({\n",
        "            \"word\": word,\n",
        "            \"category\": category,\n",
        "            \"subcategory\": subcategory,\n",
        "            \"first_position\": first_position,\n",
        "            \"last_position\": last_position\n",
        "        })\n",
        "\n",
        "    return result\n",
        "\n",
        "def find_missing_words(predicted_words, labels):\n",
        "    \"\"\"\n",
        "    Finds words present in `labels` that are not in `predicted_words`.\n",
        "\n",
        "    Args:\n",
        "        predicted_words (list): List of predicted words.\n",
        "        labels (list): List of correct words (labels).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Number of missing words and a list of those words.\n",
        "    \"\"\"\n",
        "    missing_words = [word for word in labels if word not in predicted_words]\n",
        "    return len(missing_words), missing_words\n",
        "\n",
        "def calculate_f1_score(tp, fp, fn, verbose= False):\n",
        "    \"\"\"\n",
        "    Calculate F1 Score, Recall, and Precision.\n",
        "\n",
        "    Args:\n",
        "        tp (int): True Positives.\n",
        "        fp (int): False Positives.\n",
        "        fn (int): False Negatives.\n",
        "        verbose (bool): Whether to print detailed metrics.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (f1_score, recall, precision)\n",
        "    \"\"\"\n",
        "    # Calculate precision (Total correct divided by number of attempts)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "    # Calculate recall (Total correct divided by number of words that should have been masked)\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    if verbose:\n",
        "        print('Recall:', recall)\n",
        "        print('Precision:', precision)\n",
        "        print('F1 Score:', f1_score, '\\n')\n",
        "\n",
        "    return f1_score, recall, precision\n",
        "\n",
        "def eval(extractive_pred, dict_labels, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate extractive predictions against labeled data.\n",
        "\n",
        "    Args:\n",
        "        extractive_pred (dict): Dictionary containing predicted words and categories.\n",
        "        dict_labels (dict): Dictionary of ground-truth labels for words.\n",
        "        verbose (bool): Whether to print detailed evaluation results.\n",
        "\n",
        "    Returns:\n",
        "        tuple: F1, Recall, Precision (per word and overall)\n",
        "    \"\"\"\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    correct_predicted_words = []\n",
        "    wrong_predicted_words = []\n",
        "    predicted_words = []\n",
        "    wrong_predicted_category = []\n",
        "    for pred in extractive_pred['preds']:\n",
        "\n",
        "        # predicted_words.append(pred['word'])\n",
        "        if pred['word'] in list(dict_labels.keys()):\n",
        "\n",
        "            if pred['subcategory'] == dict_labels[pred['word']]:\n",
        "                TP+=1\n",
        "                correct_predicted_words.append((pred['word'], pred['subcategory']))\n",
        "                predicted_words.append(pred['word'])\n",
        "            else:\n",
        "                FP+=1\n",
        "                wrong_predicted_category.append((pred['word'], pred['subcategory']))\n",
        "        else:\n",
        "            FP+=1\n",
        "            wrong_predicted_words.append(pred['word'])\n",
        "\n",
        "    # Calculate False Negatives\n",
        "    FN, missing_words = find_missing_words(predicted_words, list(dict_labels.keys()))\n",
        "    if verbose:\n",
        "        print('Missing words:', missing_words)\n",
        "        print('Correct Predicted words:', correct_predicted_words)\n",
        "        print('Correct word but wrong category:', wrong_predicted_category)\n",
        "        print('Wrong Predicted words:', wrong_predicted_words)\n",
        "        print('Labels:', dict_labels)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1, recall, precision = calculate_f1_score(TP, FP, FN, verbose=verbose)\n",
        "\n",
        "    return f1, recall, precision\n",
        "\n",
        "def create_generative_format(text):\n",
        "    \"\"\"\n",
        "    Convert text with tagged entities into a simplified generative format.\n",
        "\n",
        "    This function replaces each complete tag (opening and closing with content)\n",
        "    with only its opening tag, discarding the enclosed content.\n",
        "    It is useful when preparing data for generative anonymization tasks\n",
        "    where only the entity type needs to be indicated.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text containing XML-like tags with content.\n",
        "\n",
        "    Returns:\n",
        "        str: Text transformed into generative format with only opening tags.\n",
        "    \"\"\"\n",
        "    # Regex to capture the opening tag and the content between tags\n",
        "    pattern = r\"<(.*?)>(.*?)</\\1/>\"\n",
        "\n",
        "    # Replacement function that uses only the opening tag\n",
        "    def replace_match(match):\n",
        "        tag = match.group(1)\n",
        "        # Returns only the opening tag\n",
        "        return f\"<{tag}>\"\n",
        "\n",
        "    # Replace all occurrences in the text\n",
        "    replaced_text = re.sub(pattern, replace_match, text)\n",
        "    return replaced_text\n",
        "\n",
        "def fix_tags_with_replace(text):\n",
        "    \"\"\"\n",
        "    Fixes malformed tags in the text using replace for each possible case.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text generated by the model with malformed tags.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with corrected tags.\n",
        "    \"\"\"\n",
        "    # List of tags that need to be fixed\n",
        "    tags = [\n",
        "        \"AGE\", \"PHONE\", \"FAX\", \"EMAIL\", \"URL\", \"IP_ADDRESS\", \"DATE\", \"IDNUM\",\n",
        "        \"MEDICAL_RECORD\", \"DEVICE\", \"HEALTH_PLAN\", \"BIOID\", \"STREET\", \"CITY\",\n",
        "        \"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\", \"HOSPITAL\",\n",
        "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER\", \"LOCATION\"\n",
        "    ]\n",
        "\n",
        "    for tag in tags:\n",
        "        # Fix spaces around the opening tag\n",
        "        text = text.replace(f\"< {tag} >\", f\"<{tag}>\").replace(f\"< {tag}>\", f\"<{tag}>\").replace(f\"<{tag} >\", f\"<{tag}>\")\n",
        "        # Fix spaces around the closing tag\n",
        "        text = text.replace(f\"</ {tag} >\", f\"</{tag}/>\").replace(f\"</ {tag}>\", f\"</{tag}/>\").replace(f\"</{tag} >\", f\"</{tag}/>\")\n",
        "        # Fix malformed closings with extra slashes\n",
        "        text = text.replace(f\"<{tag}/> \", f\"</{tag}/>\").replace(f\"<{tag}/ >\", f\"</{tag}/>\").replace(f\"</{tag}/ >\", f\"</{tag}/>\")\n",
        "        # Remove spaces between tags and inner content\n",
        "        text = text.replace(f\"<{tag}> \", f\"<{tag}>\").replace(f\" </{tag}>\", f\"</{tag}/>\").replace(f\"</{tag}>\", f\"</{tag}/>\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def split_tags_with_multiple_words(text):\n",
        "    \"\"\"\n",
        "    Split multi-word entity tags into individual single-word tags.\n",
        "\n",
        "    This function identifies tags in the format <TAG>content</TAG/>,\n",
        "    and if the content contains multiple words, it separates them so\n",
        "    that each word is wrapped in its own <TAG>...</TAG/>.\n",
        "    If the tag content is already a single word, it remains unchanged.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text containing XML-like tags with entity labels.\n",
        "\n",
        "    Returns:\n",
        "        str: Transformed text where multi-word tags are split into single-word tags.\n",
        "    \"\"\"\n",
        "    # Regex to find tags in the format <TAG>content</TAG/>\n",
        "    pattern = r\"<(.*?)>(.*?)</\\1/>\"\n",
        "\n",
        "    # Function to replace tags with multiple words\n",
        "    def split_tag(match):\n",
        "        tag = match.group(1)  # Tag name\n",
        "        content = match.group(2)  # Content between tags\n",
        "\n",
        "        # Check if the content has multiple words\n",
        "        words = content.split()\n",
        "        if len(words) > 1:\n",
        "            # Create a new tag for each word\n",
        "            return \" \".join(f\"<{tag}>{word}</{tag}/>\" for word in words)\n",
        "        return match.group(0)  # Return the original tag if it’s a single word\n",
        "\n",
        "    # Apply regex and replace\n",
        "    return re.sub(pattern, split_tag, text)\n",
        "\n",
        "def run_prediction(input_text, model=\"gpt4-o-mini\"):\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "             Você é uma assistente que auxilia na identificação de entidades em textos médicos incluindo tags de marcação no texto original.\n",
        "             Você deve identificar somente as seguintes entidades, não crie novas entidades além das listadas aqui: \"AGE\", \"PHONE\", \"EMAIL\", \"DATE\", \"IDNUM\",\n",
        "             \"MEDICAL_RECORD\", \"HEALTH_PLAN\", \"STREET\", \"CITY\",\"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\",\n",
        "             \"HOSPITAL\", \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER. As tags possuem o seguinte formato: <PATIENT> </PATIENT/>\n",
        "             <DOCTOR> </DOCTOR/> <DATE> </DATE/>. Cada par de tags deve conter apenas uma palavra como mostram os exemplos abaixo.\"\"\"},\n",
        "\n",
        "            {\"role\": \"user\", \"content\": \"\"\"Subjetivo\\n # REVISÃO CLÍNICA - GASTROENTEROLOGIA #\\n\n",
        "              #De acordo com o Dr. Roberto Freitas, após endoscopia diagnóstica:\\n\n",
        "              - Suspender o uso de AINES temporariamente devido a risco de gastrite medicamentosa.\\n\n",
        "              - Iniciar omeprazol 40 mg diariamente por 30 dias.\\n\n",
        "              - Introduzir dieta branda, evitando alimentos condimentados e café.\\n\n",
        "              - Agendar retorno em 2 semanas para reavaliação dos sintomas.\\n\n",
        "              - Paciente Romildo Pena, 45 anos, engenheiro, instruído a procurar o serviço de emergência caso apresente vômitos ou dor intensa.\\n\n",
        "              Elaborado e assinado por médico residente Dr. Gabriel Teixeira, telefone (11) 3546-8964, CRM 634278 em 15/09/2022 09:50, consultório Rua da Saúde, 234, São Paulo.\"\"\"},\n",
        "\n",
        "            {\"role\": \"assistant\", \"content\": \"\"\"Subjetivo\\n # REVISÃO CLÍNICA - GASTROENTEROLOGIA #\\n\n",
        "              #De acordo com o Dr. <DOCTOR>Roberto</DOCTOR/> <DOCTOR>Freitas</DOCTOR/>, após endoscopia diagnóstica:\\n\n",
        "              - Suspender o uso de AINES temporariamente devido a risco de gastrite medicamentosa. \\n\n",
        "              - Iniciar omeprazol 40 mg diariamente por 30 dias. \\n\n",
        "              - Introduzir dieta branda, evitando alimentos condimentados e café.\\n\n",
        "              - Agendar retorno em 2 semanas para reavaliação dos sintomas.\\n\n",
        "              - Paciente <PATIENT>Romildo</PATIENT/> <PATIENT>Pena</PATIENT/>, <AGE>45</AGE/> anos, <PROFESSION>engenheiro</PROFESSION/>, instruído a procurar o serviço de emergência caso apresente vômitos ou dor intensa.\\n\n",
        "              Elaborado e assinado por médico residente Dr. <DOCTOR>Gabriel</DOCTOR/> <DOCTOR>Teixeira</DOCTOR/>, telefone <PHONE>(11)</PHONE/> <PHONE>3546-8964</PHONE/>, CRM <IDNUM>634278</IDNUM/> em <DATE>15</DATE/>/<DATE>09</DATE/>/<DATE>2022</DATE/> 09:50, consultório <STREET>Rua</STREET/> <STREET>da</STREET/> <STREET>Saúde</STREET/>, 234, <CITY>São</CITY/> <CITY>Paulo</CITY/>.\n",
        "              \"\"\"},\n",
        "\n",
        "            {\"role\":\"user\", \"content\": \"\"\"Subjetivo\n",
        "            ### EVOLUCAO MÉDICA - GERIATRIA #\n",
        "            # ID: Mauro José, 77 anos, natural de Paratinga-BA, procedente de Campinas-SP, casado, 4 filhos, escolaridade 4 anos, católico,\n",
        "            aposentado (açougueiro), mora com a esposa\n",
        "            #DIH: 28/08\n",
        "            #QP:\n",
        "            Piora de confusão mental e perda de força nas pernas\n",
        "            #HPMA\n",
        "            Equipe da geriatria é chamada para avaliar paciente em leito de UER.\n",
        "            Paciente acompanha em inúmeras especialidades em nossos serviços. A esposa e a filha contam que no último mês o paciente\n",
        "            começou a apresentar progressão do quadro de demência, associado a paresia de membros inferiores e de mãos, parestesia de mãos,\n",
        "            enrijecimento de articulações e dores articulares. A esposa refere que antes o paciente apresentava dificuldade para deambular, mas\n",
        "            conseguia deambular com ajuda, porém desde a última vez que retirou líquor (31/07) acabou perdendo força em MMII para deambular.\n",
        "            Associada à piora do quadro cognitivo e motor, o paciente apresentou perda do controle de esfíncteres, algo que antes era preservado.\n",
        "            A esposa disse que o paciente queixa-se de cefaleia intensa diariamente e mantém alucinações visuais e verbais. Esposa refere\n",
        "            evacuações 3x/dia, com fezes amolecidas. Urina de odor fétido e aspecto avermelhado, associado a disúria e redução do volume\n",
        "            urinário.\n",
        "            Em 2007 o paciente teve um quadro parecido sendo internado com insuficiência adrenal.\n",
        "            #AP:\n",
        "            1) Síndrome demencial moderada a grave - etiologia não esclarecida (HPN? Lewy?)\n",
        "            2) Síndrome poliglandular endócrina (ALTO ACTH/ BAIXO CORTISOL)\n",
        "            * insuficiência adrenal primária\n",
        "            * hipotireoidismo + gastrite atrófica\n",
        "            3) HAS / DLP\n",
        "            4) DRC\n",
        "            5) HBP\n",
        "            #Evolução Médica:\n",
        "            Paciente em leito de enfermaria geral, acompanhado pela filha, alerta, comunicativo e colaborativo, calmo, atenção preservada. Mantem\n",
        "            boa aceitação da dieta via oral. Baixo debito urinario nas ultimas 24hs (SVD 125ml), apresentando mais 200ml durante a visita pela\n",
        "            manhã; evacuação ausente. Segue hemodinamicamente estavel, sem intercorrencias no periodo, controles dentro da normalidade, com\n",
        "            melhora do edema. Em programação de nova sessão de hemodialise hoje a tarde.\n",
        "            #EXAME FISICO:\n",
        "            REG, descorado 2+/4 em mucosas , mucosas pouco secas, anictérico, afebril , CAM -\n",
        "            AR: MV+ sem RA, sat 96% em aa\n",
        "            ACV: BRNF 2T, extremidades quentes , FC 81 bpm, PA 130x80mmHg\n",
        "            #EXAMES COMPLEMENTARES:\n",
        "            (29/08/23): TC cranio: ventriculos aumentados bilateralmente , atrofia global com predominancia me lobos temporais, MTA 4\n",
        "            bilateralmente - impressão da nossa equipe aguardando laudo oficial\n",
        "            (01/09/23) USG Rins e Vias Urinarias: Sinais de nefropatia parenquimatoa crônica bilateral;\n",
        "            #CULTURAS:\n",
        "            (04/09/23) URC Negativo\n",
        "            (29/08/23) Uroc: Microbiota diversa (Contaminação)\n",
        "            (01/09/23) Hemocultura: aguarda resultado\n",
        "            Impressão\n",
        "            #HDs\n",
        "            1) DRC agudizada por desidratação?\n",
        "            > Realizada primeira sessão de diálise em 02/09 . HD: 02/09; 04/09; 05/09; 07/09\n",
        "            2) Hipercalemia corrigida\n",
        "            > sec a DRC agudizada\n",
        "            3) Delirium em melhora\n",
        "            > Progressão do quadro neurológico de base? Sec a agudização renal ?\n",
        "            # Impressão:\n",
        "            Paciente no momento com sinais vitais estáveis porém com piora da função renal no decorrer da internação. Equipe de nefrologia\n",
        "            segue em acompanhamento conjunto > realizada primeira dialise dia 02/09. Em programação de nova sessão de dialise hoje;\n",
        "            Conduta\n",
        "            #CDs\n",
        "            - Checo exames - piora das escórias nitrogenadas - em programação de HD hoje pela nefrologia;\n",
        "            - Checo USG Rins e Vias Urinarias: Sinais de nefropatia parenquimatoa crônica bilateral;\n",
        "            - Aguarda eletroforese de proteinas;\n",
        "            - Vigilancia de plaquetopenia (uso heparina 12/12h + uso de heparina durante dialise ??)\n",
        "            - Reavaliaremos plano terapêutico diariamente, de acordo com resposta ao tratamento instituído e considerando morbidades/\n",
        "            funcionalidade do paciente.\n",
        "            - Exames laboratorias diario e seguimento conjunto com a nefrologia.\n",
        "            - Forneço boletim medico para filha, esclareço duvidas, atualizo plano terapeutico, acolho.\n",
        "            RODOLFO R3 GERIATRIA + DR ANDRÉ FATTORI.\n",
        "            Elaborado e assinado por Dr. Rodolfo Antonio De Oliveira Nogueira, Crm 210538 em 07/09/2023 15:32\n",
        "            \"\"\"},\n",
        "\n",
        "             {\"role\":\"assistant\", \"content\": \"\"\" Subjetivo\n",
        "            ### EVOLUCAO MÉDICA - GERIATRIA #\n",
        "            # ID: <PATIENT>Mauro</PATIENT/> <PATIENT>José</PATIENT/>, <AGE>77</AGE/> anos, natural de <CITY>Paratinga</CITY/>-<STATE>BA</STATE/>, procedente de <CITY>Campinas</CITY/>-<STATE>SP</STATE/>, <OTHER>casado</OTHER/>, 4 filhos, escolaridade 4 anos, <OTHER>católico</OTHER/>,\n",
        "            <PROFESSION>aposentado</PROFESSION/> (<PROFESSION>açougueiro</PROFESSION/>), mora com a esposa\n",
        "            #DIH: <DATE>28</DATE/>/<DATE>08</DATE/>\n",
        "            #QP:\n",
        "            Piora de confusão mental e perda de força nas pernas\n",
        "            #HPMA\n",
        "            Equipe da geriatria é chamada para avaliar paciente em leito de <LOCATION_OTHER>UER</LOCATION_OTHER/>.\n",
        "            Paciente acompanha em inúmeras especialidades em nossos serviços. A esposa e a filha contam que no último mês o paciente\n",
        "            começou a apresentar progressão do quadro de demência, associado a paresia de membros inferiores e de mãos, parestesia de mãos,\n",
        "            enrijecimento de articulações e dores articulares. A esposa refere que antes o paciente apresentava dificuldade para deambular, mas\n",
        "            conseguia deambular com ajuda, porém desde a última vez que retirou líquor (<DATE>31</DATE/>/<DATE>07</DATE/>) acabou perdendo força em MMII para deambular.\n",
        "            Associada à piora do quadro cognitivo e motor, o paciente apresentou perda do controle de esfíncteres, algo que antes era preservado.\n",
        "            A esposa disse que o paciente queixa-se de cefaleia intensa diariamente e mantém alucinações visuais e verbais. Esposa refere\n",
        "            evacuações 3x/dia, com fezes amolecidas. Urina de odor fétido e aspecto avermelhado, associado a disúria e redução do volume\n",
        "            urinário.\n",
        "            Em <DATE>2007</DATE/> o paciente teve um quadro parecido sendo internado com insuficiência adrenal.\n",
        "            #AP:\n",
        "            1) Síndrome demencial moderada a grave - etiologia não esclarecida (HPN? Lewy?)\n",
        "            2) Síndrome poliglandular endócrina (ALTO ACTH/ BAIXO CORTISOL)\n",
        "            * insuficiência adrenal primária\n",
        "            * hipotireoidismo + gastrite atrófica\n",
        "            3) HAS / DLP\n",
        "            4) DRC\n",
        "            5) HBP\n",
        "            #Evolução Médica:\n",
        "            Paciente em leito de enfermaria geral, acompanhado pela filha, alerta, comunicativo e colaborativo, calmo, atenção preservada. Mantem\n",
        "            boa aceitação da dieta via oral. Baixo debito urinario nas ultimas 24hs (SVD 125ml), apresentando mais 200ml durante a visita pela\n",
        "            manhã; evacuação ausente. Segue hemodinamicamente estavel, sem intercorrencias no periodo, controles dentro da normalidade, com\n",
        "            melhora do edema. Em programação de nova sessão de hemodialise hoje a tarde.\n",
        "            #EXAME FISICO:\n",
        "            REG, descorado 2+/4 em mucosas , mucosas pouco secas, anictérico, afebril , CAM -\n",
        "            AR: MV+ sem RA, sat 96% em aa\n",
        "            ACV: BRNF 2T, extremidades quentes , FC 81 bpm, PA 130x80mmHg\n",
        "            #EXAMES COMPLEMENTARES:\n",
        "            (<DATE>29</DATE/>/<DATE>08</DATE/>/<DATE>23</DATE/>): TC cranio: ventriculos aumentados bilateralmente , atrofia global com predominancia me lobos temporais, MTA 4\n",
        "            bilateralmente - impressão da nossa equipe aguardando laudo oficial\n",
        "            (<DATE>01</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>) USG Rins e Vias Urinarias: Sinais de nefropatia parenquimatoa crônica bilateral;\n",
        "            #CULTURAS:\n",
        "            (<DATE>04</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>) URC Negativo\n",
        "            (<DATE>29</DATE/>/<DATE>08</DATE/>/<DATE>23</DATE/>) Uroc: Microbiota diversa (Contaminação)\n",
        "            (<DATE>01</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>) Hemocultura: aguarda resultado\n",
        "            Impressão\n",
        "            #HDs\n",
        "            1) DRC agudizada por desidratação?\n",
        "            > Realizada primeira sessão de diálise em <DATE>02</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>. HD: <DATE>02</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>; <DATE>04</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>; <DATE>05</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>; <DATE>07</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>\n",
        "            2) Hipercalemia corrigida\n",
        "            > sec a DRC agudizada\n",
        "            3) Delirium em melhora\n",
        "            > Progressão do quadro neurológico de base? Sec a agudização renal ?\n",
        "            # Impressão:\n",
        "            Paciente no momento com sinais vitais estáveis porém com piora da função renal no decorrer da internação. Equipe de nefrologia\n",
        "            segue em acompanhamento conjunto > realizada primeira dialise dia <DATE>02</DATE/>/<DATE>09</DATE/>/<DATE>23</DATE/>. Em programação de nova sessão de dialise hoje;\n",
        "            Conduta\n",
        "            #CDs\n",
        "            - Checo exames - piora das escórias nitrogenadas - em programação de HD hoje pela nefrologia;\n",
        "            - Checo USG Rins e Vias Urinarias: Sinais de nefropatia parenquimatoa crônica bilateral;\n",
        "            - Aguarda eletroforese de proteinas;\n",
        "            - Vigilancia de plaquetopenia (uso heparina 12/12h + uso de heparina durante dialise ??)\n",
        "            - Reavaliaremos plano terapêutico diariamente, de acordo com resposta ao tratamento instituído e considerando morbidades/\n",
        "            funcionalidade do paciente.\n",
        "            - Exames laboratorias diario e seguimento conjunto com a nefrologia.\n",
        "            - Forneço boletim medico para filha, esclareço duvidas, atualizo plano terapeutico, acolho.\n",
        "            <DOCTOR>RODOLFO</DOCTOR/> R3 GERIATRIA + DR <DOCTOR>ANDRÉ</DOCTOR/> <DOCTOR>FATTORI</DOCTOR/>.\n",
        "            Elaborado e assinado por Dr. <DOCTOR>Antonio</DOCTOR/> <DOCTOR>De</DOCTOR/> <DOCTOR>Oliveira</DOCTOR/> <DOCTOR>Nogueira</DOCTOR/>, Crm <IDNUM>210538</IDNUM/> em <DATE>07</DATE/>/<DATE>09</DATE/>/<DATE>2023</DATE/> 15:32\n",
        "            \"\"\"},\n",
        "\n",
        "            {\"role\": \"user\", \"content\": \"\"\" Subjetivo\n",
        "              # EVOLUÇÃO ENFERMARIA DE CARDIOLOGIA #\n",
        "              # ID: Antonio Alves de Paula, 58 anos\n",
        "              HC: 12735360\n",
        "              Acompanhado por Maria Ferreira de Paula\n",
        "              #HD:\n",
        "              1. Taquicardia ventricular em EE de 24/08/2023\n",
        "              História de síncopes ambulatorialmente em paciente com miocardiopatia chagásica\n",
        "              2. Hipotireoidismo induzido por tapazol\n",
        "              #AP\n",
        "              Miocardiopatia chagásica\n",
        "              Hipertireoidismo em uso de tapazol --> evolução para hipotireoidismo\n",
        "              ICFER\n",
        "              #HPMA: Paciente com miocardiopatia chagásica acompanha no ambulatório de cardiologia da Unicamp.\n",
        "              # CONDUTAS:\n",
        "              - Aguarda chegada do CDI\n",
        "              Tulio I1 + Arthur R4 + Dra. Adriana\n",
        "              Elaborado e assinado por Acadêmico Tulio Carmona Moura em 13/09/2023 22:57\n",
        "              \"\"\"},\n",
        "\n",
        "            {\"role\": \"assistant\", \"content\": \"\"\"Subjetivo\n",
        "              # EVOLUÇÃO ENFERMARIA DE CARDIOLOGIA #\n",
        "              # ID: <PATIENT>Antonio</PATIENT/> <PATIENT>Alves</PATIENT/> <PATIENT>de</PATIENT/> <PATIENT>Paula</PATIENT/>, <AGE>58</AGE/> anos\n",
        "              HC: <MEDICAL_RECORD>12735360</MEDICAL_RECORD/>\n",
        "              Acompanhado por <PATIENT>Maria</PATIENT/> <PATIENT>Ferreira</PATIENT/> <PATIENT>de</PATIENT/> <PATIENT>Paula</PATIENT/>\n",
        "              #HD:\n",
        "              1. Taquicardia ventricular em EE de <DATE>24</DATE/>/<DATE>08</DATE/>/<DATE>2023</DATE/>\n",
        "              História de síncopes ambulatorialmente em paciente com miocardiopatia chagásica\n",
        "              2. Hipotireoidismo induzido por tapazol\n",
        "              #AP\n",
        "              Miocardiopatia chagásica\n",
        "              Hipertireoidismo em uso de tapazol --> evolução para hipotireoidismo\n",
        "              ICFER\n",
        "              #HPMA: Paciente com miocardiopatia chagásica acompanha no ambulatório de cardiologia da <ORGANIZATION>Unicamp</ORGANIZATION/>.\n",
        "              # CONDUTAS:\n",
        "              - Aguarda chegada do CDI\n",
        "              <DOCTOR>Tulio</DOCTOR/> I1 + <DOCTOR>Arthur</DOCTOR/> R4 + Dra. <DOCTOR>Adriana</DOCTOR/>\n",
        "              Elaborado e assinado por Acadêmico <DOCTOR>Tulio</DOCTOR/> <DOCTOR>Carmona</DOCTOR/> <DOCTOR>Moura</DOCTOR/> em <DATE>13</DATE/>/<DATE>09</DATE/>/<DATE>2023</DATE/> 22:57\"\"\"},\n",
        "\n",
        "\n",
        "            {\"role\": \"user\", \"content\": input_text}\n",
        "\n",
        "        ]\n",
        "        response = client.chat.completions.create(\n",
        "            model = model,\n",
        "            messages = messages,\n",
        "            temperature = 0.0,\n",
        "            max_tokens = 4096,\n",
        "            top_p = 1.0,\n",
        "            frequency_penalty = 0,\n",
        "            presence_penalty = 0,\n",
        "            stop = None\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "252d0382iMZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test set\n",
        "dataset_ = load_dataset(\"Venturus/AnonyMED-BR\")\n",
        "test_set = dataset_['test']\n",
        "\n",
        "list_f1 = []\n",
        "list_recall = []\n",
        "list_precision = []\n",
        "list_pred_gen = []\n",
        "list_syn = []\n",
        "list_preds = []\n",
        "list_id = []\n",
        "for test_sample in tqdm(test_set):\n",
        "\n",
        "    # Create the sliding window to input on the NLP model\n",
        "    clean_text = re.sub(r\"<[^>]+>\", \"\", test_sample['text'])\n",
        "\n",
        "    try:\n",
        "        # Run inference on all windows and post-process data\n",
        "        windows = sliding_window((clean_text), window_size=1700, overlap=0)\n",
        "        preds = [run_prediction(window ,model=\"<path_to_model>\").choices[0].message.content for window in windows]\n",
        "        final_pred = ' '.join(preds)\n",
        "        fixed_final_pred = fix_tags_with_replace(final_pred).replace(' , ',', ')\n",
        "        fixed_final_pred = split_tags_with_multiple_words(fixed_final_pred)\n",
        "\n",
        "        #### Fix some tags if needed\n",
        "        fixed_final_pred = fixed_final_pred.replace(\"</DATE/>/<DATE>\",\"/\")\n",
        "        fixed_final_pred = fixed_final_pred.replace(\"</PHONE/>-<PHONE>\",\"-\")\n",
        "\n",
        "        ### Save prediction in a list\n",
        "        list_preds.append(fixed_final_pred)\n",
        "\n",
        "        ### Extractive Format Evaluation ###\n",
        "        tags = extract_tagged_words(fixed_final_pred)\n",
        "        extractive_pred = {'id': test_sample['id'], 'preds': tags}\n",
        "\n",
        "        # Get labels in the evaluation format\n",
        "        dict_labels = {item['word']: item['subcategory'] for item in test_sample['labels']}\n",
        "\n",
        "        # Run evaluation\n",
        "        f1, recall, precision = eval(extractive_pred, dict_labels, verbose=False)\n",
        "\n",
        "        # Save results\n",
        "        list_f1.append(f1)\n",
        "        list_recall.append(recall)\n",
        "        list_precision.append(precision)\n",
        "\n",
        "        # Save if it is synthetic or not\n",
        "        list_syn.append(test_sample['synthetic'])\n",
        "\n",
        "        # Save example id\n",
        "        list_id.append(test_sample['id'])\n",
        "\n",
        "        ### Generative Format Evaluation ###\n",
        "        list_pred_gen.append({'text': re.sub(r\"<[^>]+>\", \"\", test_sample['text']), 'masked_text':create_generative_format(test_sample['text']), 'prediction':create_generative_format(fixed_final_pred)})\n",
        "\n",
        "    except:\n",
        "        print('An error occured during model generation step', preds)\n",
        "\n",
        "\n",
        "### Create a dataframe with the evaluations\n",
        "save_df = pd.DataFrame()\n",
        "save_df['id'] = list_id\n",
        "save_df['Recall'] = list_recall\n",
        "save_df['Precision'] = list_precision\n",
        "save_df['F1'] = list_f1\n",
        "save_df['synthetic'] = list_syn\n",
        "save_df['Prediction'] = list_preds\n",
        "save_df.to_csv('GPT_results.csv', index=False)\n",
        "\n",
        "avg_f1 = sum(list_f1) / len(list_f1) if list_f1 else 0\n",
        "avg_recall = sum(list_recall) / len(list_recall) if list_recall else 0\n",
        "avg_precision = sum(list_precision) / len(list_precision) if list_precision else 0\n",
        "\n",
        "print('\\n --------------------------------------------------------------- \\n')\n",
        "print('Recall:', avg_recall)\n",
        "print('Precision:', avg_precision)\n",
        "print('F1:', avg_f1)\n",
        "\n",
        "## Save predictions on the generative format\n",
        "with open('gpt_generative_predictions.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(list_pred_gen, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "urlhXANPm31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation per Entity"
      ],
      "metadata": {
        "id": "w5OuI6mr8e9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_entities = [\"PHONE\", \"AGE\", \"FAX\", \"EMAIL\", \"URL\", \"IP_ADDRESS\", \"DATE\", \"IDNUM\",\n",
        "        \"MEDICAL_RECORD\", \"DEVICE\", \"HEALTH_PLAN\", \"BIOID\", \"STREET\", \"CITY\",\n",
        "        \"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\", \"HOSPITAL\",\n",
        "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER\", \"LOCATION\"]\n",
        "\n",
        "def eval_entity(extractive_pred, dict_labels, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of extractive predictions against reference labels for entities.\n",
        "\n",
        "    This function compares predicted words and their subcategories to a reference dictionary of labels.\n",
        "    It calculates True Positives (TP), False Positives (FP), and False Negatives (FN), and returns\n",
        "    the corresponding F1 score, recall, and precision. Optionally, it can print detailed information\n",
        "    about correct and incorrect predictions.\n",
        "\n",
        "    Args:\n",
        "        extractive_pred (dict): Dictionary containing predicted entities under the key 'preds',\n",
        "                                where each prediction is a dictionary with 'word' and 'subcategory'.\n",
        "        dict_labels (dict): Dictionary of reference labels with words as keys and subcategories as values.\n",
        "        verbose (bool, optional): If True, prints detailed information about missing words,\n",
        "                                  correct predictions, and incorrect predictions. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - f1 (float): F1 score for entity predictions.\n",
        "            - recall (float): Recall score for entity predictions.\n",
        "            - precision (float): Precision score for entity predictions.\n",
        "    \"\"\"\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    correct_predicted_words = []\n",
        "    wrong_predicted_words = []\n",
        "    predicted_words = []\n",
        "    wrong_predicted_category = []\n",
        "    for pred in extractive_pred['preds']:\n",
        "\n",
        "        if pred['word'] in list(dict_labels.keys()):\n",
        "\n",
        "            if pred['subcategory'] == dict_labels[pred['word']]:\n",
        "                TP+=1\n",
        "                correct_predicted_words.append((pred['word'], pred['subcategory']))\n",
        "                predicted_words.append(pred['word'])\n",
        "            else:\n",
        "                FP+=1\n",
        "                wrong_predicted_category.append((pred['word'], pred['subcategory']))\n",
        "        else:\n",
        "            FP+=1\n",
        "            wrong_predicted_words.append(pred['word'])\n",
        "\n",
        "    # Calculate False Negatives\n",
        "    FN, missing_words = find_missing_words(predicted_words, list(dict_labels.keys()))\n",
        "    if verbose:\n",
        "        print('Missing words:', missing_words)\n",
        "        print('Correct Predicted words:', correct_predicted_words)\n",
        "        print('Correct word but wrong category:', wrong_predicted_category)\n",
        "        print('Wrong Predicted words:', wrong_predicted_words)\n",
        "        print('Labels:', dict_labels)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1, recall, precision = calculate_f1_score(TP, FP, FN, verbose=verbose)\n",
        "\n",
        "    return f1, recall, precision"
      ],
      "metadata": {
        "id": "MeQmLGXdqzFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersect_by_id(list1, list2):\n",
        "    \"\"\"\n",
        "    Return the intersection of two lists of dictionaries based on the 'id' key.\n",
        "\n",
        "    This function filters `list1` and keeps only those dictionaries whose 'id' exists in `list2`.\n",
        "\n",
        "    Args:\n",
        "        list1 (list of dict): First list of dictionaries to filter.\n",
        "        list2 (list of dict): Second list of dictionaries used as reference for 'id' values.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list of dictionaries from `list1` whose 'id' is present in `list2`.\n",
        "    \"\"\"\n",
        "    ids2 = {item['id'] for item in list2}\n",
        "    return [item for item in list1 if item['id'] in ids2]"
      ],
      "metadata": {
        "id": "TmrzdD4qiw8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read predictions\n",
        "dict_preds = pd.read_csv('GPT_results.csv').to_dict('records')\n",
        "\n",
        "test_set = dataset_['test']\n",
        "test_set = intersect_by_id(test_set, dict_preds)\n",
        "\n",
        "list_f1_entity = []\n",
        "list_recall_entity = []\n",
        "list_precision_entity = []\n",
        "list_entity = []\n",
        "list_id = []\n",
        "list_syn_entity = []\n",
        "for test_sample, dict_pred in zip(test_set, dict_preds):\n",
        "\n",
        "    assert test_sample['id'] == dict_pred['id']\n",
        "\n",
        "    ### Extractive Format Evaluation ###\n",
        "    tags = extract_tagged_words(dict_pred['Prediction'])\n",
        "    extractive_pred = {'id': test_sample['id'], 'preds': tags}\n",
        "\n",
        "    # Get labels in the evaluation format\n",
        "    dict_labels = {item['word']: item['subcategory'] for item in test_sample['labels']}\n",
        "\n",
        "    ## Evaluate performance per entity\n",
        "    for entity in list_entities:\n",
        "        ## Filter the entity to be evaluated inside the label\n",
        "        filtered_dict_labels = {key: value for key, value in dict_labels.items() if value == entity}\n",
        "\n",
        "        ## Filter the entity to be evaluated that were predicted by the model\n",
        "        filtered_tags = [sample for sample in extractive_pred['preds'] if sample['subcategory'] == entity]\n",
        "\n",
        "        filtered_extractive_pred = {'id': test_sample['id'], 'preds': filtered_tags}\n",
        "\n",
        "        ## Check if the entity exists inside the label to run evaluation\n",
        "        if len(filtered_dict_labels) > 0:\n",
        "            f1, recall, precision = eval_entity(filtered_extractive_pred, filtered_dict_labels, verbose=False)\n",
        "\n",
        "            list_f1_entity.append(f1)\n",
        "            list_recall_entity.append(recall)\n",
        "            list_precision_entity.append(precision)\n",
        "            list_entity.append(entity)\n",
        "            list_id.append(test_sample['id'])\n",
        "\n",
        "            # Save if it is synthetic or not\n",
        "            list_syn_entity.append(test_sample['synthetic'])\n",
        "\n",
        "\n",
        "### Create a dataframe with the evaluations\n",
        "entity_save_df = pd.DataFrame()\n",
        "entity_save_df['id'] = list_id\n",
        "entity_save_df['Entity'] = list_entity\n",
        "entity_save_df['Recall'] = list_recall_entity\n",
        "entity_save_df['Precision'] = list_precision_entity\n",
        "entity_save_df['F1'] = list_f1_entity\n",
        "entity_save_df['synthetic'] = list_syn_entity\n",
        "\n",
        "entity_save_df.to_csv('GPT_entity_results.csv')\n"
      ],
      "metadata": {
        "id": "U1GBswHqlvXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving results per entity\n",
        "df_grouped_entity = entity_save_df.groupby('Entity')[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_grouped_entity.to_csv('GPT_entity_final_results.csv', index=False)\n",
        "print(df_grouped_entity)"
      ],
      "metadata": {
        "id": "5Ahw0_lqueeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving results per entity grouped by real and synthetic\n",
        "df_grouped_syn_entity = entity_save_df.groupby(['synthetic', 'Entity'])[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_grouped_syn_entity.to_csv('GPT_entity_final_grouped_results.csv', index=False)\n",
        "print(df_grouped_syn_entity)"
      ],
      "metadata": {
        "id": "mnWnmGUQtMRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving F1 scores grouped by real and synthetic samples\n",
        "df_results_ = pd.read_csv('GPT_results.csv')\n",
        "df_results_grouped = df_results_.groupby(['synthetic'])[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_results_grouped.to_csv('GPT_grouped_results.csv', index= False)"
      ],
      "metadata": {
        "id": "AHTtidVXo2pw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}