{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install datasets\n",
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "651PZAA7YAPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Read and tokenize data"
      ],
      "metadata": {
        "id": "dWGS4Z651nPw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5MzA29ihVdA-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "import re\n",
        "import spacy\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Load the model and the tokenizer\n",
        "model_name = \"<model_name>\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def fix_tags_with_replace(text):\n",
        "    \"\"\"\n",
        "    Fixes the format of tags in the text using replace for each possible case.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text generated by the model with malformed tags.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with corrected tags.\n",
        "    \"\"\"\n",
        "    # List of tags that need to be fixed\n",
        "    tags = [\n",
        "        \"AGE\", \"PHONE\", \"FAX\", \"EMAIL\", \"URL\", \"IP_ADDRESS\", \"DATE\", \"IDNUM\",\n",
        "        \"MEDICAL_RECORD\", \"DEVICE\", \"HEALTH_PLAN\", \"BIOID\", \"STREET\", \"CITY\",\n",
        "        \"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\", \"HOSPITAL\",\n",
        "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER\", \"LOCATION\"\n",
        "    ]\n",
        "\n",
        "    for tag in tags:\n",
        "        # Fix spaces around the opening tag\n",
        "        text = text.replace(f\"< {tag} >\", f\"<{tag}>\").replace(f\"< {tag}>\", f\"<{tag}>\").replace(f\"<{tag} >\", f\"<{tag}>\")\n",
        "        # Fix spaces around the closing tag\n",
        "        text = text.replace(f\"</ {tag} >\", f\"</{tag}/>\").replace(f\"</ {tag}>\", f\"</{tag}/>\").replace(f\"</{tag} >\", f\"</{tag}/>\")\n",
        "        # Fix malformed closings with extra slashes\n",
        "        text = text.replace(f\"<{tag}/> \", f\"</{tag}/>\").replace(f\"<{tag}/ >\", f\"</{tag}/>\").replace(f\"</{tag}/ >\", f\"</{tag}/>\")\n",
        "        # Remove spaces between tags and the inner content\n",
        "        text = text.replace(f\"<{tag}> \", f\"<{tag}>\").replace(f\" </{tag}>\", f\"</{tag}/>\").replace(f\"</{tag}>\", f\"</{tag}/>\")\n",
        "\n",
        "    return text\n",
        "\n",
        "def sliding_window(text, window_size=200, overlap=50):\n",
        "    \"\"\"\n",
        "    Function that splits a text into sliding windows of size `window_size` with an `overlap`.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be processed.\n",
        "        window_size (int): The maximum number of tokens per window.\n",
        "        overlap (int): The number of overlapping tokens between windows.\n",
        "\n",
        "    Returns:\n",
        "        List of str: List of texts divided into sliding windows.\n",
        "    \"\"\"\n",
        "    # Load the SpaCy model for Portuguese (or another language, if necessary)\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    # Process the full text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract tokens\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    # List to store text windows\n",
        "    windows = []\n",
        "\n",
        "    # Sliding window implementation\n",
        "    for i in range(0, len(tokens), window_size - overlap):\n",
        "        # Capture a window of tokens\n",
        "        window = tokens[i:i + window_size]\n",
        "        windows.append(fix_tags_with_replace(\" \".join(window)))\n",
        "\n",
        "        # Stop if we are at the end of the text\n",
        "        if i + window_size >= len(tokens):\n",
        "            break\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes the input and target texts to prepare them for model training.\n",
        "\n",
        "    This function processes a batch of examples containing input texts and their\n",
        "    corresponding target texts.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary containing two keys:\n",
        "            - \"input_text\": str or List[str], the source texts.\n",
        "            - \"target_text\": str or List[str], the target/label texts.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with tokenized inputs and labels in the format expected\n",
        "              by Hugging Face's Trainer API:\n",
        "              - \"input_ids\": Token IDs for the input texts.\n",
        "              - \"attention_mask\": Attention masks for the inputs.\n",
        "              - \"labels\": Token IDs for the target texts.\n",
        "    \"\"\"\n",
        "    inputs = examples[\"input_text\"]  # Input text\n",
        "    targets = examples[\"target_text\"]  # Target text\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\"  # Ensures uniform length\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (labels)\n",
        "    with tokenizer.as_target_tokenizer():  # Ensures the tokenizer is in target mode\n",
        "        labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\"\n",
        "        )[\"input_ids\"]  # Extract tokenized IDs\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_dataset = True\n",
        "\n",
        "if preprocess_dataset:\n",
        "\n",
        "    ### READ DATA\n",
        "    dataset_ = load_dataset(\"Venturus/AnonyMED-BR\")\n",
        "\n",
        "    ### PRE-PROCESS DATA\n",
        "    # Train\n",
        "    list_chunks = [chunk for train_sample in tqdm(dataset_['train']) for chunk in sliding_window(train_sample[\"text\"])]\n",
        "    train_chunks = [{'input_text': re.sub(r\"<[^>]+>\", \"\", chunk), 'target_text': chunk} for chunk in list_chunks]\n",
        "\n",
        "    with open('t5_train.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_chunks, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Eval\n",
        "    list_chunks_eval = [chunk for eval_sample in tqdm(dataset_['validation']) for chunk in sliding_window(eval_sample[\"text\"])]\n",
        "    eval_chunks = [{'input_text': re.sub(r\"<[^>]+>\", \"\", chunk), 'target_text': chunk} for chunk in list_chunks_eval]\n",
        "\n",
        "    with open('t5_eval.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(eval_chunks, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Open and read the training set\n",
        "    with open('t5_train.json', 'r') as file:\n",
        "        train_chunks = json.load(file)\n",
        "\n",
        "    # Open and read the evaluation set\n",
        "    with open('t5_eval.json', 'r') as file:\n",
        "        eval_chunks = json.load(file)\n",
        "\n",
        "# Convert the data into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_list(train_chunks)\n",
        "eval_dataset = Dataset.from_list(eval_chunks[0:500])\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": eval_dataset\n",
        "})\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Preview the data\n",
        "print(dataset)\n",
        "\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "WsJ0kHJdVok8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Fine Tuning"
      ],
      "metadata": {
        "id": "4Fx7omYZ1jti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training parameters\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"<path_to_folder>\",  # Directory to save results\n",
        "    report_to=\"none\",\n",
        "  #  evaluation_strategy=\"steps\",   # Evaluate at the end of each epoch\n",
        "    learning_rate=5e-5,             # Learning rate\n",
        "    per_device_train_batch_size=4,  # Training batch size\n",
        "    per_device_eval_batch_size=8,   # Evaluation batch size\n",
        "   # weight_decay=0.01,             # Weight decay\n",
        "    save_total_limit=3,             # Save only the last 3 checkpoints\n",
        "    num_train_epochs=3,             # Number of training epochs\n",
        "    predict_with_generate=True,     # Generate text during evaluation\n",
        "    logging_dir='logs',             # Logs directory\n",
        "    logging_steps=500,              # Log information every 500 steps\n",
        "    fp16 = False,\n",
        "    save_strategy=\"epoch\",          # Save model at each epoch\n",
        "    eval_steps=500,\n",
        "    gradient_accumulation_steps=1\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                                    # Loaded model\n",
        "    args=training_args,                             # Training configurations\n",
        "    train_dataset=tokenized_datasets[\"train\"],      # Training dataset\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],  # Validation dataset\n",
        "    tokenizer=tokenizer                             # Corresponding tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"T5_model\")\n",
        "tokenizer.save_pretrained(\"T5_model\")\n",
        "print(\"Model trained and saved successfully!\")"
      ],
      "metadata": {
        "id": "tr3ZTGfzYTWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Inference"
      ],
      "metadata": {
        "id": "clQFTSvmf96s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tagged_words(text):\n",
        "    \"\"\"\n",
        "    Extract labeled words from text based on XML-like tags.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text containing tags.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries with extracted words, their category, subcategory,\n",
        "              and character positions in the original text.\n",
        "    \"\"\"\n",
        "    dict_categories = {\"AGE\": \"AGE\", \"PHONE\": \"CONTACT\", \"FAX\": \"CONTACT\", \"EMAIL\": \"CONTACT\", \"URL\": \"CONTACT\",\n",
        "                   \"IP_ADDRESS\": \"CONTACT\", \"DATE\": \"DATE\", \"IDNUM\": \"ID\", \"MEDICAL_RECORD\": \"ID\", \"DEVICE\": \"ID\",\n",
        "                   \"HEALTH_PLAN\": \"ID\", \"BIOID\": \"ID\", \"STREET\": \"LOCATION\", \"CITY\": \"LOCATION\", \"ZIP\": \"LOCATION\",\n",
        "                   \"STATE\": \"LOCATION\", \"COUNTRY\": \"LOCATION\", \"LOCATION_OTHER\": \"LOCATION\", \"ORGANIZATION\": \"LOCATION\",\n",
        "                   \"HOSPITAL\": \"LOCATION\", \"PATIENT\": \"NAME\", \"DOCTOR\": \"NAME\", \"USERNAME\": \"NAME\", \"PROFESSION\": \"PROFESSION\",\n",
        "                   \"OTHER\": \"OTHER\", \"LOCATION\": \"LOCATION\"}\n",
        "\n",
        "    pattern = r\"<(.*?)>(.*?)</\\1/>\"  # Regex to capture tags and content inside them\n",
        "    matches = re.finditer(pattern, text)\n",
        "\n",
        "    result = []\n",
        "    for match in matches:\n",
        "        tag = match.group(1)\n",
        "        word = match.group(2)\n",
        "        first_position = match.start(2)  # Start position of the extracted word\n",
        "        last_position = match.end(2)     # End position of the extracted word\n",
        "        category = dict_categories.get(tag, \"UNKNOWN\")  # Main category\n",
        "        subcategory = tag\n",
        "\n",
        "        result.append({\n",
        "            \"word\": word,\n",
        "            \"category\": category,\n",
        "            \"subcategory\": subcategory,\n",
        "            \"first_position\": first_position,\n",
        "            \"last_position\": last_position\n",
        "        })\n",
        "\n",
        "    return result\n",
        "\n",
        "def find_missing_words(predicted_words, labels):\n",
        "    \"\"\"\n",
        "    Find words present in `labels` that are missing in `predicted_words`.\n",
        "\n",
        "    Args:\n",
        "        predicted_words (list): List of predicted words.\n",
        "        labels (list): List of ground-truth words.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Number of missing words and a list of those words.\n",
        "    \"\"\"\n",
        "    missing_words = [word for word in labels if word not in predicted_words]\n",
        "    return len(missing_words), missing_words\n",
        "\n",
        "def calculate_f1_score(tp, fp, fn, verbose=False):\n",
        "    \"\"\"\n",
        "    Calculate F1 Score, Recall, and Precision.\n",
        "\n",
        "    Args:\n",
        "        tp (int): True Positives.\n",
        "        fp (int): False Positives.\n",
        "        fn (int): False Negatives.\n",
        "        verbose (bool): Whether to print detailed metrics.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (f1_score, recall, precision)\n",
        "    \"\"\"\n",
        "    # Precision = Correct predictions divided by total predicted positives\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "    # Recall = Correct predictions divided by total actual positives\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # F1 Score = Harmonic mean of precision and recall\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    if verbose:\n",
        "        print('Recall:', recall)\n",
        "        print('Precision:', precision)\n",
        "        print('F1 Score:', f1_score, '\\n')\n",
        "\n",
        "    return f1_score, recall, precision\n",
        "\n",
        "def eval(extractive_pred, dict_labels, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate extractive predictions against labeled data.\n",
        "\n",
        "    Args:\n",
        "        extractive_pred (dict): Dictionary containing predicted words and categories.\n",
        "        dict_labels (dict): Dictionary of ground-truth labels for words.\n",
        "        verbose (bool): Whether to print detailed evaluation results.\n",
        "\n",
        "    Returns:\n",
        "        tuple: F1, Recall, Precision (per word and overall)\n",
        "    \"\"\"\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    correct_predicted_words = []\n",
        "    wrong_predicted_words = []\n",
        "    predicted_words = []\n",
        "    wrong_predicted_category = []\n",
        "    for pred in extractive_pred['preds']:\n",
        "\n",
        "        # predicted_words.append(pred['word'])\n",
        "        if pred['word'] in list(dict_labels.keys()):\n",
        "\n",
        "            if pred['subcategory'] == dict_labels[pred['word']]:\n",
        "                TP += 1\n",
        "                correct_predicted_words.append((pred['word'], pred['subcategory']))\n",
        "                predicted_words.append(pred['word'])\n",
        "            else:\n",
        "                FP += 1\n",
        "                wrong_predicted_category.append((pred['word'], pred['subcategory']))\n",
        "        else:\n",
        "            FP += 1\n",
        "            wrong_predicted_words.append(pred['word'])\n",
        "\n",
        "    # Calculate False Negatives\n",
        "    FN, missing_words = find_missing_words(predicted_words, list(dict_labels.keys()))\n",
        "    if verbose:\n",
        "        print('Missing words:', missing_words)\n",
        "        print('Correct Predicted words:', correct_predicted_words)\n",
        "        print('Correct word but wrong category:', wrong_predicted_category)\n",
        "        print('Wrong Predicted words:', wrong_predicted_words)\n",
        "        print('Labels:', dict_labels)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1, recall, precision = calculate_f1_score(TP, FP, FN, verbose=verbose)\n",
        "\n",
        "    return f1, recall, precision\n",
        "\n",
        "def create_generative_format(text):\n",
        "    \"\"\"\n",
        "    Convert text with tagged entities into a simplified generative format.\n",
        "\n",
        "    This function replaces each complete tag (opening and closing with content)\n",
        "    with only its opening tag, discarding the enclosed content.\n",
        "    It is useful when preparing data for generative anonymization tasks\n",
        "    where only the entity type needs to be indicated.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text containing XML-like tags with content.\n",
        "\n",
        "    Returns:\n",
        "        str: Text transformed into generative format with only opening tags.\n",
        "    \"\"\"\n",
        "    # Regex to capture opening tag and content between tags\n",
        "    pattern = r\"<(.*?)>(.*?)</\\1/>\"\n",
        "\n",
        "    # Replace function that keeps only the opening tag\n",
        "    def replace_match(match):\n",
        "        tag = match.group(1)\n",
        "        # Return only the opening tag\n",
        "        return f\"<{tag}>\"\n",
        "\n",
        "    # Replace all matches in the text\n",
        "    replaced_text = re.sub(pattern, replace_match, text)\n",
        "    return replaced_text\n",
        "\n",
        "\n",
        "def run_prediction(input_text, max_length=450):\n",
        "    \"\"\"\n",
        "    Run model inference on input text using a sliding window.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): The input text to be processed.\n",
        "        max_length (int): Maximum output length.\n",
        "\n",
        "    Returns:\n",
        "        list: List of decoded model predictions.\n",
        "    \"\"\"\n",
        "    # Tokenization\n",
        "    inputs = tokenizer([window for window in windows], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    if device != 'cpu':\n",
        "        # Move tensors to the correct device\n",
        "        inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "    # Batch Inference\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        do_sample=False,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    return tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "### Load fine-tuned model\n",
        "model_name = \"T5_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to the device\n",
        "if device != 'cpu':\n",
        "    model = model.to(device)\n"
      ],
      "metadata": {
        "id": "SXzVvuE-jgCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test data\n",
        "test_set = dataset_['test']\n",
        "\n",
        "list_f1 = []\n",
        "list_recall = []\n",
        "list_precision = []\n",
        "list_pred_gen = []\n",
        "list_syn = []\n",
        "list_id = []\n",
        "list_preds = []\n",
        "for test_sample in tqdm(test_set):\n",
        "\n",
        "    # Create the sliding window to input on the NLP model\n",
        "    clean_text = re.sub(r\"<[^>]+>\", \"\", test_sample['text'])\n",
        "    windows = sliding_window((clean_text), window_size=150, overlap=0)\n",
        "\n",
        "    # Run batch inference on all windows and post-process data\n",
        "    preds = run_prediction(windows, 512)\n",
        "\n",
        "    final_pred = ' '.join(preds)\n",
        "    fixed_final_pred = fix_tags_with_replace(final_pred).replace(' , ',', ').replace('</PHONE/>-<PHONE>','-').replace(' </PHONE/>','</PHONE/>').replace(' )</PHONE/>',')</PHONE/>')#.replace(' )',')').replace('( ','(')\n",
        "\n",
        "    ### Save prediction in a list\n",
        "    list_preds.append(fixed_final_pred)\n",
        "\n",
        "    ### Extractive Format Evaluation ###\n",
        "    tags = extract_tagged_words(fixed_final_pred)\n",
        "    extractive_pred = {'id': test_sample['id'], 'preds': tags}\n",
        "\n",
        "    # Get labels in the evaluation format\n",
        "    dict_labels = {item['word']: item['subcategory'] for item in test_sample['labels']}\n",
        "\n",
        "    # Run evaluation\n",
        "    f1, recall, precision = eval(extractive_pred, dict_labels, verbose=False)\n",
        "\n",
        "    # Save results for correct words and classes\n",
        "    list_f1.append(f1)\n",
        "    list_recall.append(recall)\n",
        "    list_precision.append(precision)\n",
        "\n",
        "    # Save if it is synthetic or not\n",
        "    list_syn.append(test_sample['synthetic'])\n",
        "\n",
        "    # Save example id\n",
        "    list_id.append(test_sample['id'])\n",
        "\n",
        "    ### Generative Format Evaluation ###\n",
        "    list_pred_gen.append({'text': re.sub(r\"<[^>]+>\", \"\", test_sample['text']), 'masked_text':create_generative_format(test_sample['text']), 'prediction':create_generative_format(fixed_final_pred)})\n",
        "\n",
        "\n",
        "### Create a dataframe with the evaluations\n",
        "save_df = pd.DataFrame()\n",
        "save_df['id'] = list_id\n",
        "save_df['Recall'] = list_recall\n",
        "save_df['Precision'] = list_precision\n",
        "save_df['F1'] = list_f1\n",
        "save_df['synthetic'] = list_syn\n",
        "save_df['Prediction'] = list_preds\n",
        "save_df.to_csv('T5_results.csv', index = False)\n",
        "\n",
        "avg_f1 = sum(list_f1) / len(list_f1) if list_f1 else 0\n",
        "avg_recall = sum(list_recall) / len(list_recall) if list_recall else 0\n",
        "avg_precision = sum(list_precision) / len(list_precision) if list_precision else 0\n",
        "\n",
        "print('Recall:', avg_recall)\n",
        "print('Precision:', avg_precision)\n",
        "print('F1:', avg_f1)\n",
        "\n",
        "## Save predictions on the generative format\n",
        "with open('t5_generative_predictions.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(list_pred_gen, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "f5VcieaWVEC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation per entity"
      ],
      "metadata": {
        "id": "B6DZj_eflC3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_entities = [\"PHONE\", \"AGE\", \"FAX\", \"EMAIL\", \"URL\", \"IP_ADDRESS\", \"DATE\", \"IDNUM\",\n",
        "        \"MEDICAL_RECORD\", \"DEVICE\", \"HEALTH_PLAN\", \"BIOID\", \"STREET\", \"CITY\",\n",
        "        \"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\", \"HOSPITAL\",\n",
        "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER\", \"LOCATION\"]\n",
        "\n",
        "def eval_entity(extractive_pred, dict_labels, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of extractive predictions against reference labels for entities.\n",
        "\n",
        "    This function compares predicted words and their subcategories to a reference dictionary of labels.\n",
        "    It calculates True Positives (TP), False Positives (FP), and False Negatives (FN), and returns\n",
        "    the corresponding F1 score, recall, and precision. Optionally, it can print detailed information\n",
        "    about correct and incorrect predictions.\n",
        "\n",
        "    Args:\n",
        "        extractive_pred (dict): Dictionary containing predicted entities under the key 'preds',\n",
        "                                where each prediction is a dictionary with 'word' and 'subcategory'.\n",
        "        dict_labels (dict): Dictionary of reference labels with words as keys and subcategories as values.\n",
        "        verbose (bool, optional): If True, prints detailed information about missing words,\n",
        "                                  correct predictions, and incorrect predictions. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - f1 (float): F1 score for entity predictions.\n",
        "            - recall (float): Recall score for entity predictions.\n",
        "            - precision (float): Precision score for entity predictions.\n",
        "    \"\"\"\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    correct_predicted_words = []\n",
        "    wrong_predicted_words = []\n",
        "    predicted_words = []\n",
        "    wrong_predicted_category = []\n",
        "    for pred in extractive_pred['preds']:\n",
        "\n",
        "      #  predicted_words.append(pred['word'])\n",
        "        if pred['word'] in list(dict_labels.keys()):\n",
        "\n",
        "            if pred['subcategory'] == dict_labels[pred['word']]:\n",
        "                TP+=1\n",
        "                correct_predicted_words.append((pred['word'], pred['subcategory']))\n",
        "                predicted_words.append(pred['word'])\n",
        "            else:\n",
        "                FP+=1\n",
        "                wrong_predicted_category.append((pred['word'], pred['subcategory']))\n",
        "        else:\n",
        "            FP+=1\n",
        "            wrong_predicted_words.append(pred['word'])\n",
        "\n",
        "    # Calculate False Negatives\n",
        "    FN, missing_words = find_missing_words(predicted_words, list(dict_labels.keys()))\n",
        "    if verbose:\n",
        "        print('Missing words:', missing_words)\n",
        "        print('Correct Predicted words:', correct_predicted_words)\n",
        "        print('Correct word but wrong category:', wrong_predicted_category)\n",
        "        print('Wrong Predicted words:', wrong_predicted_words)\n",
        "        print('Labels:', dict_labels)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1, recall, precision = calculate_f1_score(TP, FP, FN, verbose=verbose)\n",
        "\n",
        "    return f1, recall, precision"
      ],
      "metadata": {
        "id": "g4gcnqhvaNku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read predictions\n",
        "dict_preds = pd.read_csv('T5_results.csv').to_dict('records')\n",
        "\n",
        "test_set = dataset_['test']\n",
        "\n",
        "list_f1_entity = []\n",
        "list_recall_entity = []\n",
        "list_precision_entity = []\n",
        "list_entity = []\n",
        "list_id = []\n",
        "list_syn_entity = []\n",
        "for test_sample, dict_pred in zip(test_set, dict_preds):\n",
        "\n",
        "    assert test_sample['id'] == dict_pred['id']\n",
        "\n",
        "    ### Extractive Format Evaluation ###\n",
        "    tags = extract_tagged_words(dict_pred['Prediction'])\n",
        "    extractive_pred = {'id': test_sample['id'], 'preds': tags}\n",
        "\n",
        "    # Get labels in the evaluation format\n",
        "    dict_labels = {item['word']: item['subcategory'] for item in test_sample['labels']}\n",
        "\n",
        "    ## Evaluate performance per entity\n",
        "    for entity in list_entities:\n",
        "        ## Filter the entity to be evaluated inside the label\n",
        "        filtered_dict_labels = {key: value for key, value in dict_labels.items() if value == entity}\n",
        "\n",
        "        ## Filter the entity to be evaluated that were predicted by the model\n",
        "        filtered_tags = [sample for sample in extractive_pred['preds'] if sample['subcategory'] == entity]\n",
        "\n",
        "        filtered_extractive_pred = {'id': test_sample['id'], 'preds': filtered_tags}\n",
        "\n",
        "        ## Check if the entity exists inside the label to run evaluation\n",
        "        if len(filtered_dict_labels) > 0:\n",
        "            f1, recall, precision = eval_entity(filtered_extractive_pred, filtered_dict_labels, verbose=False)\n",
        "\n",
        "            list_f1_entity.append(f1)\n",
        "            list_recall_entity.append(recall)\n",
        "            list_precision_entity.append(precision)\n",
        "            list_entity.append(entity)\n",
        "            list_id.append(test_sample['id'])\n",
        "\n",
        "            # Save if it is synthetic or not\n",
        "            list_syn_entity.append(test_sample['synthetic'])\n",
        "\n",
        "### Create a dataframe with the evaluations\n",
        "entity_save_df = pd.DataFrame()\n",
        "entity_save_df['id'] = list_id\n",
        "entity_save_df['Entity'] = list_entity\n",
        "entity_save_df['Recall'] = list_recall_entity\n",
        "entity_save_df['Precision'] = list_precision_entity\n",
        "entity_save_df['F1'] = list_f1_entity\n",
        "entity_save_df['synthetic'] = list_syn_entity\n",
        "\n",
        "entity_save_df.to_csv('T5_entity_results.csv')"
      ],
      "metadata": {
        "id": "YejI-7cNqC6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate results grouped per Entity\n",
        "df_grouped_entity = entity_save_df.groupby('Entity')[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_grouped_entity.to_csv('T5_entity_final_results.csv', index=False)\n",
        "print(df_grouped_entity)"
      ],
      "metadata": {
        "id": "TBl-H9kxnO9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate results grouped per Entity but separated between real and synthetic samples\n",
        "df_grouped_syn_entity = entity_save_df.groupby(['synthetic', 'Entity'])[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_grouped_syn_entity.to_csv('T5_entity_final_grouped_results.csv', index=False)\n",
        "print(df_grouped_syn_entity)"
      ],
      "metadata": {
        "id": "bF6X6sfCnjrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate F1 scores grouped by real and synthetic samples\n",
        "df_results_ = pd.read_csv('T5_results.csv')\n",
        "df_results_grouped = df_results_.groupby(['synthetic'])[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "\n",
        "df_results_grouped.to_csv('T5_grouped_results.csv', index= False)"
      ],
      "metadata": {
        "id": "io-tNBjEcXDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}