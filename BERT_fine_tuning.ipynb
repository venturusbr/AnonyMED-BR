{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37uZxd4KZ1rX"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install datasets\n",
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!pip install -q evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Prepare dataset to NER format"
      ],
      "metadata": {
        "id": "jWMg8PUE6sB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, ClassLabel, Sequence, Features, Value, DatasetDict\n",
        "import json\n",
        "import spacy\n",
        "import re\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def sliding_window(text, window_size=200, overlap=50):\n",
        "    \"\"\"\n",
        "    Splits a text into sliding windows of size `window_size` with an `overlap`.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be processed.\n",
        "        window_size (int): The maximum number of tokens per window.\n",
        "        overlap (int): The number of overlapping tokens between windows.\n",
        "\n",
        "    Returns:\n",
        "        List of str: List of texts divided into sliding windows.\n",
        "    \"\"\"\n",
        "    # Load the SpaCy model for Portuguese (or another language, if needed)\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "    # Process the entire text with SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract tokens\n",
        "    tokens = [token.text for token in doc]\n",
        "\n",
        "    # List to store the text windows\n",
        "    windows = []\n",
        "\n",
        "    # Sliding window implementation\n",
        "    for i in range(0, len(tokens), window_size - overlap):\n",
        "        # Capture a window of tokens\n",
        "        window = tokens[i:i + window_size]\n",
        "        windows.append(fix_tags_with_replace(\" \".join(window)))\n",
        "\n",
        "        # Stop if we are at the end of the text\n",
        "        if i + window_size >= len(tokens):\n",
        "            break\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def fix_tags_with_replace(text):\n",
        "    \"\"\"\n",
        "    Fixes the format of tags in the text using string replacement for all possible cases.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text generated by the model containing malformed tags.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with corrected tags.\n",
        "    \"\"\"\n",
        "    # List of tags that need to be fixed\n",
        "    tags = [\n",
        "        \"AGE\", \"PHONE\", \"FAX\", \"EMAIL\", \"URL\", \"IP_ADDRESS\", \"DATE\", \"IDNUM\",\n",
        "        \"MEDICAL_RECORD\", \"DEVICE\", \"HEALTH_PLAN\", \"BIOID\", \"STREET\", \"CITY\",\n",
        "        \"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\", \"HOSPITAL\",\n",
        "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER\", \"LOCATION\"\n",
        "    ]\n",
        "\n",
        "    for tag in tags:\n",
        "        # Fix spaces around the opening tag\n",
        "        text = text.replace(f\"< {tag} >\", f\"<{tag}>\").replace(f\"< {tag}>\", f\"<{tag}>\").replace(f\"<{tag} >\", f\"<{tag}>\")\n",
        "        # Fix spaces around the closing tag\n",
        "        text = text.replace(f\"</ {tag} >\", f\"</{tag}/>\").replace(f\"</ {tag}>\", f\"</{tag}/>\").replace(f\"</{tag} >\", f\"</{tag}/>\")\n",
        "        # Fix malformed closing with extra slashes\n",
        "        text = text.replace(f\"<{tag}/> \", f\"</{tag}/>\").replace(f\"<{tag}/ >\", f\"</{tag}/>\").replace(f\"</{tag}/ >\", f\"</{tag}/>\")\n",
        "        # Remove spaces between tags and inner content\n",
        "        text = text.replace(f\"<{tag}> \", f\"<{tag}>\").replace(f\" </{tag}>\", f\"</{tag}/>\").replace(f\"</{tag}>\", f\"</{tag}/>\")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def label_ner_format(strings, tag_values):\n",
        "    \"\"\"\n",
        "    Converts a list of strings into a sequence of numeric labels for NER tasks.\n",
        "\n",
        "    Each string is checked against the provided `tag_values` dictionary. If a tag\n",
        "    is found in the string, its corresponding numeric value is assigned.\n",
        "    If the same tag appears consecutively, its value is incremented by 1.\n",
        "    Strings without tags are labeled with 0.\n",
        "\n",
        "    Args:\n",
        "        strings (list of str): List of strings containing text with possible tags.\n",
        "        tag_values (dict): A dictionary mapping tags to numeric values.\n",
        "\n",
        "    Returns:\n",
        "        list of int: A list of numeric labels corresponding to the detected tags.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    previous_tag = None  # Track the last seen tag\n",
        "\n",
        "    for string in strings:\n",
        "        # Extract the tag if it exists in the current string\n",
        "        current_tag = next((tag for tag in tag_values if tag in string), None)\n",
        "\n",
        "        if current_tag:\n",
        "            # If the tag matches the previous one, increment its value\n",
        "            if current_tag == previous_tag:\n",
        "                value = tag_values[current_tag] + 1\n",
        "            else:\n",
        "                value = tag_values[current_tag]\n",
        "\n",
        "            previous_tag = current_tag  # Update the previous tag\n",
        "            result.append(value)\n",
        "        else:\n",
        "            result.append(0)  # No tag, assign 0\n",
        "            previous_tag = current_tag  # Update the previous tag\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Define labels to be used as entities\n",
        "labels = [\n",
        "    'O',  # No entity\n",
        "    'B-AGE', 'I-AGE',\n",
        "    'B-PHONE', 'I-PHONE',\n",
        "    'B-EMAIL', 'I-EMAIL',\n",
        "    'B-DATE', 'I-DATE',\n",
        "    'B-IDNUM', 'I-IDNUM',\n",
        "    'B-MEDICAL_RECORD', 'I-MEDICAL_RECORD',\n",
        "    'B-HEALTH_PLAN', 'I-HEALTH_PLAN',\n",
        "    'B-STREET', 'I-STREET',\n",
        "    'B-CITY', 'I-CITY',\n",
        "    'B-ZIP', 'I-ZIP',\n",
        "    'B-STATE', 'I-STATE',\n",
        "    'B-COUNTRY', 'I-COUNTRY',\n",
        "    'B-LOCATION_OTHER', 'I-LOCATION_OTHER',\n",
        "    'B-ORGANIZATION', 'I-ORGANIZATION',\n",
        "    'B-HOSPITAL', 'I-HOSPITAL',\n",
        "    'B-PATIENT', 'I-PATIENT',\n",
        "    'B-DOCTOR', 'I-DOCTOR',\n",
        "    'B-PROFESSION', 'I-PROFESSION',\n",
        "    'B-OTHER', 'I-OTHER'\n",
        "]\n",
        "\n",
        "# Create the entity2id dictionary\n",
        "entity2id = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "# Create the tag2id dictionary\n",
        "tag2id = {\"<\"+tag.split('-')[1]+\">\": idx for idx, tag in enumerate(labels[1:])}\n",
        "\n",
        "tag2id, entity2id"
      ],
      "metadata": {
        "id": "Cnvpoc51fYGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_tags(text):\n",
        "    # Transform <DATE>dd/mm/yyyy</DATE/> and <DATE>dd/mm/yy</DATE/>\n",
        "    text = re.sub(\n",
        "        r'<DATE>(\\d{2})/(\\d{2})/(\\d{2,4})</DATE/>',\n",
        "        r'<DATE>\\1</DATE/> <DATE>/</DATE/> <DATE>\\2</DATE/> <DATE>/</DATE/> <DATE>\\3</DATE/>',\n",
        "        text\n",
        "    )\n",
        "    # Transform <DATE>dd/mm</DATE/>\n",
        "    text = re.sub(\n",
        "        r'<DATE>(\\d{2})/(\\d{2,4})</DATE/>',\n",
        "        r'<DATE>\\1</DATE/> <DATE>/</DATE/> <DATE>\\2</DATE/>',\n",
        "        text\n",
        "    )\n",
        "    # Transform <PHONE>(21)</PHONE/>  → ( <PHONE>21</PHONE/> )\n",
        "    text = re.sub(\n",
        "        r'<PHONE>\\((\\d{2})\\)</PHONE/>',\n",
        "        r'( <PHONE>\\1</PHONE/> )',\n",
        "        text\n",
        "    )\n",
        "    # Transform <PHONE>99856-7421</PHONE/>  → <PHONE>99856</PHONE/> <PHONE>-</PHONE/> <PHONE>7421</PHONE/>\n",
        "    text = re.sub(\n",
        "        r'<PHONE>(\\d{4,5})-(\\d{4})</PHONE/>',\n",
        "        r'<PHONE>\\1</PHONE/> <PHONE>-</PHONE/> <PHONE>\\2</PHONE/>',\n",
        "        text\n",
        "    )\n",
        "    # Transform <EMAIL>marcia.silva@gmail.com</EMAIL/> → <EMAIL>marcia.silva</EMAIL/> <EMAIL>@</EMAIL/> <EMAIL>gmail.com</EMAIL/>\n",
        "    text = re.sub(\n",
        "        r'<EMAIL>([\\w\\.-]+)@([\\w\\.-]+)</EMAIL/>',\n",
        "        r'<EMAIL>\\1</EMAIL/> <EMAIL>@</EMAIL/> <EMAIL>\\2</EMAIL/>',\n",
        "        text\n",
        "    )\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "HDvonfPjNkDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_data = True\n",
        "\n",
        "if preprocess_data:\n",
        "\n",
        "    ### READ DATA\n",
        "    dataset_ = load_dataset(\"Venturus/AnonyMED-BR\")\n",
        "\n",
        "    ### PRE-PROCESS DATA\n",
        "    # Train\n",
        "    list_chunks = [\" \".join(chunk.replace('\\n', '').split()).strip() for train_sample in tqdm(dataset_['train']) for chunk in sliding_window(split_tags(train_sample[\"text\"]))]\n",
        "    train_chunks = [{'id':idx,\n",
        "                    'tokens': re.sub(r\"<[^>]+>\", \"\", chunk.replace(\"/>-<\", \"/> - <\")).split(' '),\n",
        "                    'ner_tags': label_ner_format(chunk.replace(\"/>-<\", \"/> - <\").split(' '), tag2id)}\n",
        "                    for idx, chunk in enumerate(list_chunks)]\n",
        "\n",
        "    # Save intermediary step\n",
        "    with open('bert_train.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_chunks, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Eval\n",
        "    list_chunks_eval = [\" \".join(chunk.replace('\\n', '').split()).strip() for eval_sample in tqdm(dataset_['validation']) for chunk in sliding_window(split_tags(eval_sample[\"text\"]))]\n",
        "    eval_chunks = [{'id':idx,\n",
        "                    'tokens': re.sub(r\"<[^>]+>\", \"\", chunk.replace(\"/>-<\", \"/> - <\")).split(' '),\n",
        "                    'ner_tags': label_ner_format(chunk.replace(\"/>-<\", \"/> - <\").split(' '), tag2id)}\n",
        "                    for idx, chunk in enumerate(list_chunks_eval)]\n",
        "\n",
        "    # Save intermediary step\n",
        "    with open('bert_eval.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(eval_chunks, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Open and read the training set\n",
        "    with open('bert_train.json', 'r') as file:\n",
        "        train_chunks = json.load(file)\n",
        "\n",
        "    # Open and read the evaluation set\n",
        "    with open('bert_eval.json', 'r') as file:\n",
        "        eval_chunks = json.load(file)\n",
        "\n",
        "\n",
        "# Create ClassLabel structure\n",
        "ner_tag_feature = ClassLabel(names=labels)\n",
        "\n",
        "# Create the dataset schema\n",
        "features = Features({\n",
        "    'id': Value(dtype='string'),  # Identificador único para cada amostra\n",
        "    'tokens': Sequence(Value(dtype='string')),  # Lista de tokens\n",
        "    'ner_tags': Sequence(ner_tag_feature)  # Lista de rótulos alinhados aos tokens\n",
        "})\n",
        "\n",
        "# Convert the data into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_list(train_chunks, features=features)\n",
        "eval_dataset = Dataset.from_list(eval_chunks[0:500], features=features)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": eval_dataset\n",
        "})\n",
        "\n",
        "# Map ids to labels to load a BERT model with correct output head\n",
        "tag_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
        "id2label = dict(enumerate(tag_names))\n",
        "label2id = dict(zip(id2label.values(), id2label.keys()))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "ao-Y1dzP6jsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Tokenize data"
      ],
      "metadata": {
        "id": "IlKsDEcEKhUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Load model\n",
        "model_name = \"google-bert/bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def tokenize_and_align_tags(records):\n",
        "    \"\"\"\n",
        "    Tokenizes input text and aligns Named Entity Recognition (NER) tags with the resulting tokens.\n",
        "\n",
        "    This function handles cases where words are split into multiple subtokens by the tokenizer\n",
        "    (e.g., \"ChatGPT\" → [\"Chat\", \"##G\", \"##PT\"]). For such cases, the first subtoken inherits the\n",
        "    original tag, while subsequent subtokens are ignored by assigning them the label `-100`.\n",
        "    Special tokens (e.g., [CLS], [SEP], [PAD]) are also assigned `-100` to exclude them from loss\n",
        "    calculation during training.\n",
        "\n",
        "    Args:\n",
        "        records (dict): A dictionary containing:\n",
        "            - \"tokens\" (list of list of str): The tokenized input sentences (split by words).\n",
        "            - \"ner_tags\" (list of list of int): The corresponding NER tags for each word.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - Tokenized inputs (as produced by the Hugging Face tokenizer).\n",
        "            - \"labels\" (list of list of int): The aligned NER tags, where `-100` is used for\n",
        "              subtokens and special tokens.\n",
        "    \"\"\"\n",
        "    tokenized_results = tokenizer(records[\"tokens\"], truncation=True, is_split_into_words=True,\n",
        "                                  padding=\"max_length\", max_length=512)\n",
        "\n",
        "    input_tags_list = []\n",
        "\n",
        "    # Iterate through each set of tags in the records.\n",
        "    for i, given_tags in enumerate(records[\"ner_tags\"]):\n",
        "        # Get the word IDs corresponding to each token. This tells us to which original word each token corresponds.\n",
        "        word_ids = tokenized_results.word_ids(batch_index=i)\n",
        "\n",
        "        previous_word_id = None\n",
        "        input_tags = []\n",
        "\n",
        "        # For each token, determine which tag it should get.\n",
        "        for wid in word_ids:\n",
        "            # If the token does not correspond to any word (e.g., it's a special token), set its tag to -100.\n",
        "            if wid is None:\n",
        "                input_tags.append(-100)\n",
        "            # If the token corresponds to a new word, use the tag for that word.\n",
        "            elif wid != previous_word_id:\n",
        "                input_tags.append(given_tags[wid])\n",
        "            # If the token is a subtoken (i.e., part of a word we've already tagged), set its tag to -100.\n",
        "            else:\n",
        "                input_tags.append(-100)\n",
        "            previous_word_id = wid\n",
        "\n",
        "        input_tags_list.append(input_tags)\n",
        "\n",
        "    # Add the assigned tags to the tokenized results.\n",
        "    # In the Hugging Face Transformers library, a model recognizes the labels parameter\n",
        "    # for computing losses along with logits (predictions)\n",
        "    tokenized_results[\"labels\"] = input_tags_list\n",
        "\n",
        "    return tokenized_results\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_tags, batched=True)\n",
        "\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "esv9vLnnrzo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Fine-Tuning"
      ],
      "metadata": {
        "id": "S8nmXsSAbbZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    # p is the results containing a list of predictions and a list of labels\n",
        "    # Unpack the predictions and true labels from the input tuple 'p'.\n",
        "    predictions_list, labels_list = p\n",
        "\n",
        "    # Convert the raw prediction scores into tag indices by selecting the tag with the highest score for each token.\n",
        "    predictions_list = np.argmax(predictions_list, axis=2)\n",
        "\n",
        "    # Filter out the '-100' labels that were used to ignore certain tokens (like sub-tokens or special tokens).\n",
        "    # Convert the numeric tags in 'predictions' and 'labels' back to their string representation using 'tag_names'.\n",
        "    # Only consider tokens that have tags different from '-100'.\n",
        "    true_predictions = [\n",
        "        [tag_names[p] for (p, l) in zip(predictions, labels) if l != -100]\n",
        "        for predictions, labels in zip(predictions_list, labels_list)\n",
        "    ]\n",
        "    true_tags = [\n",
        "        [tag_names[l] for (p, l) in zip(predictions, labels) if l != -100]\n",
        "        for predictions, labels in zip(predictions_list, labels_list)\n",
        "    ]\n",
        "\n",
        "    # Evaluate the predictions using the 'seqeval' library, which is commonly used for sequence labeling tasks like NER.\n",
        "    # This provides metrics like precision, recall, and F1 score for sequence labeling tasks.\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_tags)\n",
        "\n",
        "    # Return the evaluated metrics as a dictionary.\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name, num_labels=len(id2label), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"<path_to_output>\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "   # weight_decay=0.01,\n",
        "   # evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_dir='logs',\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"epoch\",\n",
        "    #load_best_model_at_end=True,\n",
        "    fp16 = False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"BERT_model\")\n",
        "tokenizer.save_pretrained(\"BERT_model\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "lnZ_bMXALJb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Inference"
      ],
      "metadata": {
        "id": "BWrBMpWuct0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_case_majority(s):\n",
        "    \"\"\"\n",
        "    Normalize the case of a string based on majority rule.\n",
        "\n",
        "    The function counts uppercase and lowercase characters in the string.\n",
        "    - If uppercase letters are the majority, the entire string is converted to uppercase.\n",
        "    - If lowercase letters are the majority, the entire string is converted to lowercase.\n",
        "    - If counts are equal, the string is converted to lowercase.\n",
        "\n",
        "    Args:\n",
        "        s (str): Input string.\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized string in either uppercase or lowercase.\n",
        "    \"\"\"\n",
        "    upper_count = sum(1 for c in s if c.isupper())\n",
        "    lower_count = sum(1 for c in s if c.islower())\n",
        "\n",
        "    if upper_count > lower_count:\n",
        "        return s.upper()\n",
        "    elif lower_count > upper_count:\n",
        "        return s.lower()\n",
        "    else:\n",
        "        return s.lower()\n",
        "\n",
        "\n",
        "def insert_email_com(entities):\n",
        "    \"\"\"\n",
        "    Insert missing \".com\" or equivalent into detected email entities and merge them.\n",
        "\n",
        "    This function scans through a list of entities, identifies incomplete\n",
        "    email patterns (like \"example@\" or \"example.\") and ensures the domain\n",
        "    suffix \".com\" (or \"com\") is correctly appended. After insertion, it merges\n",
        "    consecutive \"EMAIL\" entities into a single complete email.\n",
        "\n",
        "    Args:\n",
        "        entities (list): A list of entity dictionaries containing 'entity',\n",
        "                         'word', 'start', 'end', and related metadata.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of entities with corrected email addresses.\n",
        "    \"\"\"\n",
        "    new_entities = []\n",
        "    for i, ent in enumerate(entities):\n",
        "        new_entities.append(ent)\n",
        "\n",
        "        # Check if the current element is I-EMAIL\n",
        "        if ent['entity'] == 'I-EMAIL':\n",
        "            # Check if there is a previous and next element\n",
        "            prev_ent = entities[i - 1] if i > 0 else None\n",
        "            next_ent = entities[i + 1] if i + 1 < len(entities) else None\n",
        "\n",
        "            # Boundary condition\n",
        "            if (\n",
        "                prev_ent is not None and\n",
        "                prev_ent['entity'] == 'I-EMAIL' and\n",
        "                (next_ent is None or next_ent['entity'] != 'I-EMAIL')\n",
        "            ):\n",
        "                # Decide what to add\n",
        "                if ent['word'] == '.':\n",
        "                    word_to_add = 'com'\n",
        "                elif ent['word'].lower() == 'com':\n",
        "                    word_to_add = ''\n",
        "                else:\n",
        "                    word_to_add = '.com'\n",
        "\n",
        "                # Calculate start and end\n",
        "                start = ent['end']\n",
        "                end = start + len(word_to_add)\n",
        "\n",
        "                # Create new element\n",
        "                new_element = {\n",
        "                    'entity': 'I-EMAIL',\n",
        "                    'score': 1.0,\n",
        "                    'index': ent['index'] + 0.1,  # temporary for ordering\n",
        "                    'word': word_to_add,\n",
        "                    'start': start,\n",
        "                    'end': end\n",
        "                }\n",
        "\n",
        "                new_entities.append(new_element)\n",
        "\n",
        "    # Step 2: Merge sequences of B-EMAIL and I-EMAIL\n",
        "    merged_entities = []\n",
        "    i = 0\n",
        "    while i < len(new_entities):\n",
        "        ent = new_entities[i]\n",
        "\n",
        "        if ent['entity'] in ['B-EMAIL', 'I-EMAIL']:\n",
        "            # Start group\n",
        "            start_idx = i\n",
        "            words = [ent['word']]\n",
        "            start_pos = ent['start']\n",
        "            end_pos = ent['end']\n",
        "            scores = [ent['score']]\n",
        "\n",
        "            # Continue while sequence of EMAIL\n",
        "            i += 1\n",
        "            while i < len(new_entities) and new_entities[i]['entity'] in ['B-EMAIL', 'I-EMAIL']:\n",
        "                words.append(new_entities[i]['word'])\n",
        "                end_pos = new_entities[i]['end']\n",
        "                scores.append(new_entities[i]['score'])\n",
        "                i += 1\n",
        "\n",
        "            merged_entity = {\n",
        "                'entity': 'B-EMAIL',\n",
        "                'score': sum(scores) / len(scores),\n",
        "                'index': new_entities[start_idx]['index'],\n",
        "                'word': check_case_majority(''.join(words)),\n",
        "                'start': start_pos,\n",
        "                'end': end_pos\n",
        "            }\n",
        "            merged_entities.append(merged_entity)\n",
        "\n",
        "        else:\n",
        "            # Keep other entities unchanged\n",
        "            merged_entities.append(ent)\n",
        "            i += 1\n",
        "\n",
        "    return merged_entities\n",
        "\n",
        "\n",
        "def merge_time_entities(entities, tag):\n",
        "    \"\"\"\n",
        "    Merge consecutive temporal entities into a single DATE entity.\n",
        "\n",
        "    This function combines sequential entities of type `tag` (e.g., \"DATE\")\n",
        "    into one continuous entity when they are adjacent in the text.\n",
        "\n",
        "    Args:\n",
        "        entities (list): A list of entity dictionaries.\n",
        "        tag (str): The entity tag to merge (e.g., \"DATE\").\n",
        "\n",
        "    Returns:\n",
        "        list: A list of entities with merged temporal expressions.\n",
        "    \"\"\"\n",
        "    merged_entities = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(entities):\n",
        "        current = entities[i]\n",
        "        if tag in current['entity']:\n",
        "            # Initialize a buffer to accumulate related temporal elements\n",
        "            merged_word = current['word']\n",
        "            start = current['start']\n",
        "            index = current['index']\n",
        "            end = current['end']\n",
        "\n",
        "            # Traverse next elements to merge DATE-related fields\n",
        "            while i + 1 < len(entities) and tag in entities[i + 1]['entity'] and entities[i]['end'] == entities[i + 1]['start']:\n",
        "                i += 1\n",
        "                merged_word += entities[i]['word']\n",
        "                end = entities[i]['end']\n",
        "\n",
        "            # Add the merged element to the output list\n",
        "            merged_entities.append({\n",
        "                'entity': 'B-{}'.format(tag),\n",
        "                'score': current['score'],  # Could be adjusted to an average if needed\n",
        "                'index': index,\n",
        "                'word': merged_word,\n",
        "                'start': start,\n",
        "                'end': end\n",
        "            })\n",
        "        else:\n",
        "            # Keep non-temporal elements unchanged\n",
        "            merged_entities.append(current)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return merged_entities\n",
        "\n",
        "\n",
        "def merge_subwords(entities):\n",
        "    \"\"\"\n",
        "    Merge subword tokens into complete words.\n",
        "\n",
        "    Subwords are typically indicated by the \"##\" prefix (e.g., \"token\" + \"##ization\").\n",
        "    This function concatenates subwords to their preceding word entity, updates\n",
        "    positions, and adjusts scores accordingly.\n",
        "\n",
        "    Args:\n",
        "        entities (list): List of entity dictionaries containing 'word', 'start', 'end', and 'score'.\n",
        "\n",
        "    Returns:\n",
        "        list: Entities with merged subwords into full words.\n",
        "    \"\"\"\n",
        "    merged_entities = []\n",
        "    temp_entity = None\n",
        "\n",
        "    for entity in entities:\n",
        "        if entity[\"word\"].startswith(\"##\"):\n",
        "            # Concatenate with the previous entity\n",
        "            if temp_entity:\n",
        "                temp_entity[\"word\"] += entity[\"word\"][2:]  # Remove \"##\" before concatenation\n",
        "                temp_entity[\"end\"] = entity[\"end\"]  # Update 'end' position\n",
        "                temp_entity[\"score\"] = min(temp_entity[\"score\"], entity[\"score\"])  # Use lowest score\n",
        "        else:\n",
        "            # Save the previous entity before starting a new one\n",
        "            if temp_entity:\n",
        "                merged_entities.append(temp_entity)\n",
        "            # Start a new entity\n",
        "            temp_entity = entity.copy()\n",
        "\n",
        "    # Add the last processed entity\n",
        "    if temp_entity:\n",
        "        merged_entities.append(temp_entity)\n",
        "\n",
        "    return merged_entities\n",
        "\n",
        "\n",
        "def replace_with_entities(text, entities):\n",
        "    \"\"\"\n",
        "    Replace words in the text with their corresponding entity tags.\n",
        "\n",
        "    Words identified as entities are substituted by their tag format `<ENTITY>`.\n",
        "    Replacement is performed in reverse order to maintain index consistency.\n",
        "\n",
        "    Args:\n",
        "        text (str): Original text.\n",
        "        entities (list): List of entities with 'start', 'end', 'word', and 'entity' fields.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with entities replaced by their tag markers.\n",
        "    \"\"\"\n",
        "    for entity in reversed(entities):  # Iterate over entities in reverse order\n",
        "        start = entity[\"start\"]\n",
        "        end = entity[\"end\"]\n",
        "        entity_type = entity[\"entity\"].split('-')[1]\n",
        "\n",
        "        # Check if the substring matches the entity word\n",
        "        if text[start:end] != entity['word']:\n",
        "            print(f\"Warning: Word mismatch. Expected '{entity['word']}', found '{text[start:end]}' at position {start}-{end}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Replace in text\n",
        "        replacement = f\"<{entity_type}>\"\n",
        "        text = text[:start] + replacement + text[end:]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def create_generative_format(text):\n",
        "    \"\"\"\n",
        "    Convert a text with annotated tags into a generative format.\n",
        "\n",
        "    This function replaces each annotated tag of the form <TAG>word</TAG/>\n",
        "    with a simplified generative format <TAG>, removing the inner content.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text containing tagged entities.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with entities in generative format.\n",
        "    \"\"\"\n",
        "    # Regex to capture opening tag and content\n",
        "    pattern = r\"<(.*?)>(.*?)</\\1/>\"\n",
        "\n",
        "    # Replacement function that keeps only the opening tag\n",
        "    def replace_match(match):\n",
        "        tag = match.group(1)\n",
        "        return f\"<{tag}>\"\n",
        "\n",
        "    # Replace all occurrences in the text\n",
        "    replaced_text = re.sub(pattern, replace_match, text)\n",
        "    return replaced_text\n",
        "\n",
        "\n",
        "def find_missing_words(predicted_words, labels):\n",
        "    \"\"\"\n",
        "    Identify missing words from predictions compared to labels.\n",
        "\n",
        "    Args:\n",
        "        predicted_words (list): List of predicted words.\n",
        "        labels (list): List of ground-truth label words.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Number of missing words, list of missing words).\n",
        "    \"\"\"\n",
        "    missing_words = [word for word in labels if word not in predicted_words]\n",
        "    return len(missing_words), missing_words\n",
        "\n",
        "\n",
        "def calculate_f1_score(tp, fp, fn, verbose=False):\n",
        "    \"\"\"\n",
        "    Calculate F1-score, recall, and precision.\n",
        "\n",
        "    Args:\n",
        "        tp (int): True positives.\n",
        "        fp (int): False positives.\n",
        "        fn (int): False negatives.\n",
        "        verbose (bool, optional): If True, prints metrics.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (f1_score, recall, precision)\n",
        "    \"\"\"\n",
        "    # Precision = True positives / all predicted positives\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "    # Recall = True positives / all actual positives\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # F1 Score = Harmonic mean of precision and recall\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    if verbose:\n",
        "        print('Recall:', recall)\n",
        "        print('Precision:', precision)\n",
        "        print('F1 Score:', f1_score, '\\n')\n",
        "\n",
        "    return f1_score, recall, precision\n",
        "\n",
        "\n",
        "def eval(extractive_pred, dict_labels, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate entity extraction predictions against ground-truth labels.\n",
        "\n",
        "    The function compares predicted entities with reference labels,\n",
        "    computes precision, recall, and F1-score, and provides insights\n",
        "    on correct predictions, incorrect categories, and missing words.\n",
        "\n",
        "    Args:\n",
        "        extractive_pred (dict): Dictionary with predictions (must include 'preds').\n",
        "        dict_labels (dict): Ground-truth labels mapping words to entity categories.\n",
        "        verbose (bool, optional): If True, prints detailed evaluation results.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (f1, recall, precision)\n",
        "    \"\"\"\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    correct_predicted_words = []\n",
        "    wrong_predicted_words = []\n",
        "    predicted_words = []\n",
        "    wrong_predicted_category = []\n",
        "    for pred in extractive_pred['preds']:\n",
        "\n",
        "        if pred['word'] in list(dict_labels.keys()):\n",
        "\n",
        "            if pred['entity'].split('-')[1] == dict_labels[pred['word']]:\n",
        "                TP+=1\n",
        "                correct_predicted_words.append((pred['word'], pred['entity'].split('-')[1]))\n",
        "                predicted_words.append(pred['word'])\n",
        "            else:\n",
        "                FP+=1\n",
        "                wrong_predicted_category.append((pred['word'], pred['entity'].split('-')[1]))\n",
        "        else:\n",
        "            FP+=1\n",
        "            wrong_predicted_words.append(pred['word'])\n",
        "\n",
        "    # Calculate False Negatives\n",
        "    FN, missing_words = find_missing_words(predicted_words, list(dict_labels.keys()))\n",
        "    if verbose:\n",
        "        print('Missing words:', missing_words)\n",
        "        print('Correct Predicted words:', correct_predicted_words)\n",
        "        print('Correct word but wrong category:', wrong_predicted_category)\n",
        "        print('Wrong Predicted words:', wrong_predicted_words)\n",
        "        print('Labels:', dict_labels)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1, recall, precision = calculate_f1_score(TP, FP, FN, verbose=verbose)\n",
        "\n",
        "    return f1, recall, precision\n",
        "\n",
        "\n",
        "def insert_intermediate_element(data, tag, sep):\n",
        "    \"\"\"\n",
        "    Insert an intermediate separator entity between consecutive digit entities.\n",
        "\n",
        "    This function ensures numeric entities that should contain separators\n",
        "    (e.g., phone numbers or IDs with hyphens/dots) are completed with\n",
        "    the given separator character.\n",
        "\n",
        "    Args:\n",
        "        data (list): List of entity dictionaries.\n",
        "        tag (str): Tag suffix to match (e.g., \"PHONE\").\n",
        "        sep (str): Separator string to insert.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of entities with inserted separators.\n",
        "    \"\"\"\n",
        "    updated_data = []\n",
        "    for i in range(len(data)):\n",
        "        current = data[i]\n",
        "        updated_data.append(current)\n",
        "\n",
        "        # Check if current and next entity match the condition\n",
        "        if (\n",
        "            current[\"entity\"].endswith(tag) and\n",
        "            current[\"word\"].isdigit() and\n",
        "            i + 1 < len(data) and\n",
        "            data[i + 1][\"entity\"].endswith(tag) and\n",
        "            data[i + 1][\"word\"].isdigit() and\n",
        "            current[\"end\"] + 1 == data[i + 1][\"start\"]\n",
        "        ):\n",
        "            # Create the intermediate element\n",
        "            intermediate_element = {\n",
        "                \"entity\": \"I-{}\".format(tag),\n",
        "                \"score\": (current[\"score\"] + data[i + 1][\"score\"]) / 2,\n",
        "                \"index\": current[\"index\"] + 1,  # Intermediate index\n",
        "                \"word\": sep,  # Insert separator\n",
        "                \"start\": current[\"end\"],\n",
        "                \"end\": data[i + 1][\"start\"]\n",
        "            }\n",
        "            updated_data.append(intermediate_element)\n",
        "\n",
        "    return updated_data\n",
        "\n",
        "\n",
        "def insert_phone_closing_parenthesis(entities):\n",
        "    \"\"\"\n",
        "    Insert a missing closing parenthesis in phone number entities.\n",
        "\n",
        "    This function identifies phone numbers where the opening parenthesis \"(\"\n",
        "    is present, followed by a two-digit code, but the closing parenthesis \")\"\n",
        "    is missing. It inserts the closing parenthesis in the correct position.\n",
        "\n",
        "    Args:\n",
        "        entities (list): List of entity dictionaries.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of entities with corrected phone numbers.\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    while i < len(entities) - 2:\n",
        "        if (entities[i]['entity'] == 'B-PHONE' and entities[i]['word'] == '(' and\n",
        "            entities[i + 1]['entity'] == 'B-PHONE' and len(entities[i + 1]['word']) == 2 and\n",
        "            (i + 2 >= len(entities) or entities[i + 2]['word'] != ')')):\n",
        "\n",
        "            # Create the new entity for ')'\n",
        "            new_entity = {\n",
        "                'entity': 'B-PHONE',\n",
        "                'score': entities[i]['score'],  # Using score of '(' for consistency\n",
        "                'index': entities[i + 1]['index'] + 1,\n",
        "                'word': ')',\n",
        "                'start': entities[i + 1]['end'],\n",
        "                'end': entities[i + 1]['end'] + 1\n",
        "            }\n",
        "\n",
        "            # Insert new entity after the second analyzed element (i + 2)\n",
        "            entities.insert(i + 2, new_entity)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return entities\n",
        "\n",
        "model_name = \"BERT_model\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eiY_L5u1Rze",
        "outputId": "cd2831ef-7db3-473e-80f3-0df2c660893c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test data\n",
        "test_set = dataset_['test']\n",
        "\n",
        "list_f1 = []\n",
        "list_recall = []\n",
        "list_precision = []\n",
        "list_pred_gen = []\n",
        "list_syn = []\n",
        "list_preds = []\n",
        "list_id = []\n",
        "for test_sample in tqdm(test_set):\n",
        "\n",
        "    # Create the sliding window to input on the NLP model\n",
        "    clean_text = re.sub(r\"<[^>]+>\", \"\", test_sample['text'])\n",
        "    windows = sliding_window((clean_text), window_size=150, overlap=0)\n",
        "\n",
        "    full_masked_text = ''\n",
        "    tags = []\n",
        "    for window in windows:\n",
        "\n",
        "        # Fix \"(\" and \")\" in sliding window function\n",
        "        window = window.replace(\"( \",\"(\").replace(\" )\",\")\")\n",
        "\n",
        "        # Run NLP model and merge tokens\n",
        "        preds = merge_subwords(classifier(window))\n",
        "\n",
        "        ## Fix missing \"/\" on DATE predictions\n",
        "        preds = insert_intermediate_element(preds,\"DATE\", \"/\")\n",
        "        ## FIX missing \"-\" on PHONE predictions\n",
        "        preds = insert_intermediate_element(preds, \"PHONE\", \"-\")\n",
        "        ## FIX missing \")\" on PHONE predictions\n",
        "        preds = insert_phone_closing_parenthesis(preds)\n",
        "        ## FIX missing \"com\" on EMAIL predictions\n",
        "        preds = insert_email_com(preds)\n",
        "\n",
        "        # Merge DATE and PHONE tags\n",
        "        preds = merge_time_entities(preds, 'DATE')\n",
        "        preds = merge_time_entities(preds, 'PHONE')\n",
        "\n",
        "        # Mask the entities on original text\n",
        "        masked_text = replace_with_entities(window, preds)\n",
        "\n",
        "        # Save a list of dicts containing predictions on extractive format\n",
        "        tags += preds\n",
        "\n",
        "        # Save text to be used in generative format\n",
        "        full_masked_text += masked_text\n",
        "\n",
        "    ### Save prediction in a list\n",
        "    list_preds.append(tags)\n",
        "\n",
        "    ### Extractive Format Evaluation ###\n",
        "    extractive_pred = {'id': test_sample['id'], 'preds': tags}\n",
        "\n",
        "    # Get labels in the evaluation format\n",
        "\n",
        "    dict_labels = {(re.sub(r'[()]', '', item['word'])\n",
        "                  if item['subcategory'] == 'PHONE' and re.fullmatch(r'\\(\\d{2}\\)', item['word'])\n",
        "                  else item['word']\n",
        "                  ): item['subcategory']\n",
        "                  for item in test_sample['labels']}\n",
        "\n",
        "    # Run evaluation\n",
        "    f1, recall, precision = eval(extractive_pred, dict_labels, verbose=True)\n",
        "\n",
        "    # Save results for correct words and classes\n",
        "    list_f1.append(f1)\n",
        "    list_recall.append(recall)\n",
        "    list_precision.append(precision)\n",
        "\n",
        "    # Save if it is synthetic or not\n",
        "    list_syn.append(test_sample['synthetic'])\n",
        "\n",
        "    # Save example id\n",
        "    list_id.append(test_sample['id'])\n",
        "\n",
        "    ### Generative Format Evaluation ###\n",
        "    list_pred_gen.append({'text': re.sub(r\"<[^>]+>\", \"\", test_sample['text']), 'masked_text':create_generative_format(test_sample['text']), 'prediction':full_masked_text})\n",
        "\n",
        "### Create a dataframe with the evaluations\n",
        "save_df = pd.DataFrame()\n",
        "save_df['id'] = list_id\n",
        "save_df['Recall'] = list_recall\n",
        "save_df['Precision'] = list_precision\n",
        "save_df['F1'] = list_f1\n",
        "save_df['synthetic'] = list_syn\n",
        "save_df['Prediction'] = list_preds\n",
        "save_df.to_csv('BERT_results.csv', index=False)\n",
        "\n",
        "### Save on the JSON format for evaluate by entity\n",
        "save_df.to_json('BERT_results.json', orient=\"records\", lines=True)\n",
        "\n",
        "avg_f1 = sum(list_f1) / len(list_f1) if list_f1 else 0\n",
        "avg_recall = sum(list_recall) / len(list_recall) if list_recall else 0\n",
        "avg_precision = sum(list_precision) / len(list_precision) if list_precision else 0\n",
        "\n",
        "print('Recall:', avg_recall)\n",
        "print('Precision:', avg_precision)\n",
        "print('F1:', avg_f1)\n",
        "\n",
        "## Save predictions on the generative format\n",
        "with open('/bert_generative_predictions.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(list_pred_gen, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "_Ixz0zXY1RmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation per entity"
      ],
      "metadata": {
        "id": "2pS0H6oTb6Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_entities = [\"PHONE\", \"AGE\", \"FAX\", \"EMAIL\", \"URL\", \"IP_ADDRESS\", \"DATE\", \"IDNUM\",\n",
        "        \"MEDICAL_RECORD\", \"DEVICE\", \"HEALTH_PLAN\", \"BIOID\", \"STREET\", \"CITY\",\n",
        "        \"ZIP\", \"STATE\", \"COUNTRY\", \"LOCATION_OTHER\", \"ORGANIZATION\", \"HOSPITAL\",\n",
        "        \"PATIENT\", \"DOCTOR\", \"USERNAME\", \"PROFESSION\", \"OTHER\", \"LOCATION\"]\n",
        "\n",
        "def eval_entity(extractive_pred, dict_labels, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of extractive predictions against reference labels for entities.\n",
        "\n",
        "    This function compares predicted words and their subcategories to a reference dictionary of labels.\n",
        "    It calculates True Positives (TP), False Positives (FP), and False Negatives (FN), and returns\n",
        "    the corresponding F1 score, recall, and precision. Optionally, it can print detailed information\n",
        "    about correct and incorrect predictions.\n",
        "\n",
        "    Args:\n",
        "        extractive_pred (dict): Dictionary containing predicted entities under the key 'preds',\n",
        "                                where each prediction is a dictionary with 'word' and 'subcategory'.\n",
        "        dict_labels (dict): Dictionary of reference labels with words as keys and subcategories as values.\n",
        "        verbose (bool, optional): If True, prints detailed information about missing words,\n",
        "                                  correct predictions, and incorrect predictions. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - f1 (float): F1 score for entity predictions.\n",
        "            - recall (float): Recall score for entity predictions.\n",
        "            - precision (float): Precision score for entity predictions.\n",
        "    \"\"\"\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    correct_predicted_words = []\n",
        "    wrong_predicted_words = []\n",
        "    predicted_words = []\n",
        "    wrong_predicted_category = []\n",
        "    for pred in extractive_pred['preds']:\n",
        "\n",
        "        if pred['word'] in list(dict_labels.keys()):\n",
        "\n",
        "            if pred['entity'].split('-')[1] == dict_labels[pred['word']]:\n",
        "                TP+=1\n",
        "                correct_predicted_words.append((pred['word'], pred['entity'].split('-')[1]))\n",
        "                predicted_words.append(pred['word'])\n",
        "            else:\n",
        "                FP+=1\n",
        "                wrong_predicted_category.append((pred['word'], pred['entity'].split('-')[1]))\n",
        "        else:\n",
        "            FP+=1\n",
        "            wrong_predicted_words.append(pred['word'])\n",
        "\n",
        "    # Calculate False Negatives\n",
        "    FN, missing_words = find_missing_words(predicted_words, list(dict_labels.keys()))\n",
        "    if verbose:\n",
        "        print('Missing words:', missing_words)\n",
        "        print('Correct Predicted words:', correct_predicted_words)\n",
        "        print('Correct word but wrong category:', wrong_predicted_category)\n",
        "        print('Wrong Predicted words:', wrong_predicted_words)\n",
        "        print('Labels:', dict_labels)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1, recall, precision = calculate_f1_score(TP, FP, FN, verbose=verbose)\n",
        "\n",
        "    return f1, recall, precision"
      ],
      "metadata": {
        "id": "cmUeEA_1b_JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = dataset_['test']\n",
        "\n",
        "## Read predictions\n",
        "dict_preds = pd.read_json('BERT_results.json', dtype={\"id\": str}, orient=\"records\", lines=True).to_dict('records')\n",
        "\n",
        "list_f1_entity = []\n",
        "list_recall_entity = []\n",
        "list_precision_entity = []\n",
        "list_entity = []\n",
        "list_id = []\n",
        "list_syn_entity = []\n",
        "for test_sample, dict_pred in zip(test_set, dict_preds):\n",
        "\n",
        "    assert str(test_sample['id']) == dict_pred['id']\n",
        "\n",
        "    ### Extractive Format Evaluation ###\n",
        "    extractive_pred = {'id': test_sample['id'], 'preds': dict_pred['Prediction']}\n",
        "\n",
        "    # Get labels in the evaluation format\n",
        "    dict_labels = {(re.sub(r'[()]', '', item['word'])\n",
        "                  if item['subcategory'] == 'PHONE' and re.fullmatch(r'\\(\\d{2}\\)', item['word'])\n",
        "                  else item['word']\n",
        "                  ): item['subcategory']\n",
        "                  for item in test_sample['labels']}\n",
        "\n",
        "    ## Evaluate performance per entity\n",
        "    for entity in list_entities:\n",
        "        ## Filter the entity to be evaluated inside the label\n",
        "        filtered_dict_labels = {key: value for key, value in dict_labels.items() if value == entity}\n",
        "\n",
        "        ## Filter the entity to be evaluated that were predicted by the model\n",
        "        filtered_tags = [sample for sample in extractive_pred['preds'] if sample['entity'].split('-')[1] == entity]\n",
        "\n",
        "        filtered_extractive_pred = {'id': test_sample['id'], 'preds': filtered_tags}\n",
        "\n",
        "        ## Check if the entity exists inside the label to run evaluation\n",
        "        if len(filtered_dict_labels) > 0:\n",
        "            f1, recall, precision = eval_entity(filtered_extractive_pred, filtered_dict_labels, verbose=False)\n",
        "\n",
        "            list_f1_entity.append(f1)\n",
        "            list_recall_entity.append(recall)\n",
        "            list_precision_entity.append(precision)\n",
        "            list_entity.append(entity)\n",
        "            list_id.append(test_sample['id'])\n",
        "\n",
        "            # Save if it is synthetic or not\n",
        "            list_syn_entity.append(test_sample['synthetic'])\n",
        "\n",
        "### Create a dataframe with the evaluations\n",
        "entity_save_df = pd.DataFrame()\n",
        "entity_save_df['id'] = list_id\n",
        "entity_save_df['Entity'] = list_entity\n",
        "entity_save_df['Recall'] = list_recall_entity\n",
        "entity_save_df['Precision'] = list_precision_entity\n",
        "entity_save_df['F1'] = list_f1_entity\n",
        "entity_save_df['synthetic'] = list_syn_entity\n",
        "\n",
        "entity_save_df.to_csv('BERT_entity_results.csv')"
      ],
      "metadata": {
        "id": "QPjA4YEIukQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate results grouped per Entity\n",
        "df_grouped_entity = entity_save_df.groupby('Entity')[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_grouped_entity.to_csv('BERT_entity_final_results.csv', index=False)\n",
        "print(df_grouped_entity)"
      ],
      "metadata": {
        "id": "ck8DKkpZt2rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate results grouped per Entity but separated between real and synthetic samples\n",
        "df_grouped_syn_entity = entity_save_df.groupby(['synthetic', 'Entity'])[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_grouped_syn_entity.to_csv('BERT_entity_final_grouped_results.csv', index=False)\n",
        "print(df_grouped_syn_entity)"
      ],
      "metadata": {
        "id": "t7_qgR0Lt15J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate F1 scores grouped by real and synthetic samples\n",
        "df_results_ = pd.read_csv('BERT_results.csv')\n",
        "df_results_grouped = df_results_.groupby(['synthetic'])[['Precision', 'Recall', 'F1']].mean().reset_index()\n",
        "df_results_grouped.to_csv('BERT_grouped_results.csv', index= False)"
      ],
      "metadata": {
        "id": "WD9Dh_Q87417"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}